{"visited": ["https://nvlabs.github.io/cub/", "https://docs.nvidia.com/cuda/", "https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html", "https://nvidia.github.io/libcudacxx/", "https://developer.nvidia.com/nvidia-video-codec-sdk"], "data": [{"url": "https://docs.nvidia.com/cuda/", "content": "\n\n\n\n\nCUDA Toolkit Documentation 12.5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelease Notes\nCUDA Features Archive\nEULA\n\nInstallation Guides\n\nQuick Start Guide\nInstallation Guide Windows\nInstallation Guide Linux\n\nProgramming Guides\n\nProgramming Guide\nBest Practices Guide\nMaxwell Compatibility Guide\nPascal Compatibility Guide\nVolta Compatibility Guide\nTuring Compatibility Guide\nNVIDIA Ampere GPU Architecture Compatibility Guide\nHopper Compatibility Guide\nAda Compatibility Guide\nMaxwell Tuning Guide\nPascal Tuning Guide\nVolta Tuning Guide\nTuring Tuning Guide\nNVIDIA Ampere GPU Architecture Tuning Guide\nHopper Tuning Guide\nAda Tuning Guide\nPTX ISA\nVideo Decoder\nPTX Interoperability\nInline PTX Assembly\n\nCUDA API References\n\nCUDA Runtime API\nCUDA Driver API\nCUDA Math API\ncuBLAS\ncuDLA API\nNVBLAS\nnvJPEG\ncuFFT\nCUB\nCUDA C++ Standard Library\ncuFile API Reference Guide\ncuRAND\ncuSPARSE\nNPP\nnvJitLink\nnvFatbin\nNVRTC (Runtime Compilation)\nThrust\ncuSOLVER\n\nPTX Compiler API References\n\nPTX Compiler APIs\n\nMiscellaneous\n\nCUDA Demo Suite\nCUDA on WSL\nCUDA on EFLOW\nMulti-Instance GPU (MIG)\nCUDA Compatibility\nCUPTI\nDebugger API\nGPUDirect RDMA\nGPUDirect Storage\nvGPU\n\nTools\n\nNVCC\nCUDA-GDB\nCompute Sanitizer\nNsight Eclipse Plugins Installation Guide\nNsight Eclipse Plugins Edition\nNsight Systems\nNsight Compute\nNsight Visual Studio Edition\nProfiler\nCUDA Binary Utilities\n\nWhite Papers\n\nFloating Point and IEEE 754\nIncomplete-LU and Cholesky Preconditioned Iterative Methods\n\nApplication Notes\n\nCUDA for Tegra\n\nCompiler SDK\n\nlibNVVM API\nlibdevice User\u2019s Guide\nNVVM IR\n\n\n\n\n\n\nlanding\n\n\n\n\n\n \u00bb\nCUDA Toolkit Documentation 12.5 Update 1\n\n\n\nCUDA Toolkit Archive\r\n                  -\r\n                 \r\n                  Send Feedback\n\n\n\n\u00a0\n\n\n\n\n\n\n\nCUDA Toolkit Documentation 12.5 Update 1\uf0c1\nDevelop, Optimize and Deploy GPU-Accelerated Apps\nThe NVIDIA\u00ae CUDA\u00ae Toolkit provides a development environment for creating high performance GPU-accelerated\r\napplications. With the CUDA Toolkit, you can develop, optimize, and deploy your applications on GPU-accelerated\r\nembedded systems, desktop workstations, enterprise data centers, cloud-based platforms and HPC supercomputers.\r\nThe toolkit includes GPU-accelerated libraries, debugging and optimization tools, a C/C++ compiler, and a runtime\r\nlibrary to deploy your application.\nUsing built-in capabilities for distributing computations across multi-GPU configurations, scientists and researchers\r\ncan develop applications that scale from single GPU workstations to cloud installations with thousands of GPUs.\n\n\nRelease NotesThe Release Notes for the CUDA Toolkit.\n\nCUDA Features ArchiveThe list of CUDA features by release.\n\nEULAThe CUDA Toolkit End User License Agreement applies to the NVIDIA CUDA Toolkit, the NVIDIA CUDA Samples, the NVIDIA Display Driver, NVIDIA Nsight tools (Visual Studio Edition), and the associated documentation on CUDA APIs, programming model and development tools. If you do not agree with the terms and conditions of the license agreement, then do not download or use the software.\n\n\n\n\nInstallation Guides\uf0c1\n\nQuick Start GuideThis guide provides the minimal first-steps instructions for installation and verifying CUDA on a standard system.\n\nInstallation Guide WindowsThis guide discusses how to install and check for correct operation of the CUDA Development Tools on Microsoft Windows systems.\n\nInstallation Guide LinuxThis guide discusses how to install and check for correct operation of the CUDA Development Tools on GNU/Linux systems.\n\n\n\n\n\nProgramming Guides\uf0c1\n\nProgramming GuideThis guide provides a detailed discussion of the CUDA programming model and programming interface. It then describes the hardware implementation, and provides guidance on how to achieve maximum performance. The appendices include a list of all CUDA-enabled devices, detailed description of all extensions to the C++ language, listings of supported mathematical functions, C++ features supported in host and device code, details on texture fetching, technical specifications of various devices, and concludes by introducing the low-level driver API.\n\nBest Practices GuideThis guide presents established parallelization and optimization techniques and explains coding metaphors and idioms that can greatly simplify programming for CUDA-capable GPU architectures. The intent is to provide guidelines for obtaining the best performance from NVIDIA GPUs using the CUDA Toolkit.\n\nMaxwell Compatibility GuideThis application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Maxwell Architecture. This document provides guidance to ensure that your software applications are compatible with Maxwell.\n\nPascal Compatibility GuideThis application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Pascal Architecture. This document provides guidance to ensure that your software applications are compatible with Pascal.\n\nVolta Compatibility GuideThis application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Volta Architecture. This document provides guidance to ensure that your software applications are compatible with Volta.\n\nTuring Compatibility GuideThis application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Turing Architecture. This document provides guidance to ensure that your software applications are compatible with Turing.\n\nNVIDIA Ampere GPU Architecture Compatibility GuideThis application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Ampere GPU Architecture. This document provides guidance to ensure that your software applications are compatible with NVIDIA Ampere GPU architecture.\n\nHopper Compatibility GuideThis application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on the Hopper GPUs. This document provides guidance to ensure that your software applications are compatible with Hopper architecture.\n\nAda Compatibility GuideThis application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on the Ada GPUs. This document provides guidance to ensure that your software applications are compatible with Ada architecture.\n\nMaxwell Tuning GuideMaxwell is NVIDIA\u2019s 4th-generation architecture for CUDA compute applications. Applications that follow the best practices for the Kepler architecture should typically see speedups on the Maxwell architecture without any code changes. This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Maxwell architectural features.\n\nPascal Tuning GuidePascal is NVIDIA\u2019s 5th-generation architecture for CUDA compute applications. Applications that follow the best practices for the Maxwell architecture should typically see speedups on the Pascal architecture without any code changes. This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Pascal architectural features.\n\nVolta Tuning GuideVolta is NVIDIA\u2019s 6th-generation architecture for CUDA compute applications. Applications that follow the best practices for the Pascal architecture should typically see speedups on the Volta architecture without any code changes. This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Volta architectural features.\n\nTuring Tuning GuideTuring is NVIDIA\u2019s 7th-generation architecture for CUDA compute applications. Applications that follow the best practices for the Pascal architecture should typically see speedups on the Turing architecture without any code changes. This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Turing architectural features.\n\nNVIDIA Ampere GPU Architecture Tuning GuideNVIDIA Ampere GPU Architecture is NVIDIA\u2019s 8th-generation architecture for CUDA compute applications. Applications that follow the best practices for the NVIDIA Volta architecture should typically see speedups on the NVIDIA Ampere GPU Architecture without any code changes. This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging NVIDIA Ampere GPU Architecture\u2019s features.\n\nHopper Tuning GuideHopper GPU Architecture is NVIDIA\u2019s 9th-generation architecture for CUDA compute applications. Applications that follow the best practices for the NVIDIA Volta architecture should typically see speedups on the Hopper GPU Architecture without any code changes. This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Hopper GPU Architecture\u2019s features.\n\nAda Tuning GuideThe NVIDIA\u00ae Ada GPU architecture is NVIDIA\u2019s latest architecture for CUDA\u00ae compute applications. The NVIDIA Ada GPU architecture retains and extends the same CUDA programming model provided by previous NVIDIA GPU architectures such as NVIDIA Ampere and Turing, and applications that follow the best practices for those architectures should typically see speedups on the NVIDIA Ada architecture without any code changes. This guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging the NVIDIA Ada GPU architecture\u2019s features.\n\nPTX ISAThis guide provides detailed instructions on the use of PTX, a low-level parallel thread execution virtual machine and instruction set architecture (ISA). PTX exposes the GPU as a data-parallel computing device.\n\nVideo DecoderNVIDIA Video Decoder (NVCUVID) is deprecated. Instead, use the NVIDIA Video Codec SDK (https://developer.nvidia.com/nvidia-video-codec-sdk).\n\nPTX InteroperabilityThis document shows how to write PTX that is ABI-compliant and interoperable with other CUDA code.\n\nInline PTX AssemblyThis document shows how to inline PTX (parallel thread execution) assembly language statements into CUDA code. It describes available assembler statement parameters and constraints, and the document also provides a list of some pitfalls that you may encounter.\n\n\n\n\n\nCUDA API References\uf0c1\n\nCUDA Runtime APIFields in structures might appear in order that is different from the order of declaration.\n\nCUDA Driver APIFields in structures might appear in order that is different from the order of declaration.\n\nCUDA Math APIThe CUDA math API.\n\ncuBLASThe cuBLAS library is an implementation of BLAS (Basic Linear Algebra Subprograms) on top of the NVIDIA CUDA runtime. It allows the user to access the computational resources of NVIDIA Graphical Processing Unit (GPU), but does not auto-parallelize across multiple GPUs.\n\ncuDLA APIThe cuDLA API.\n\nNVBLASThe NVBLAS library is a multi-GPUs accelerated drop-in BLAS (Basic Linear Algebra Subprograms) built on top of the NVIDIA cuBLAS Library.\n\nnvJPEGThe nvJPEG Library provides high-performance GPU accelerated JPEG decoding functionality for image formats commonly used in deep learning and hyperscale multimedia applications.\n\ncuFFTThe cuFFT library user guide.\n\nCUBThe user guide for CUB.\n\nCUDA C++ Standard LibraryThe API reference for libcu++, the CUDA C++ standard library.\n\ncuFile API Reference GuideThe NVIDIA\u00ae GPUDirect\u00ae Storage cuFile API Reference Guide provides information about the preliminary version of the cuFile API reference guide that is used in applications and frameworks to leverage GDS technology and describes the intent, context, and operation of those APIs, which are part of the GDS technology.\n\ncuRANDThe cuRAND library user guide.\n\ncuSPARSEThe cuSPARSE library user guide.\n\nNPPNVIDIA NPP is a library of functions for performing CUDA accelerated processing. The initial set of functionality in the library focuses on imaging and video processing and is widely applicable for developers in these areas. NPP will evolve over time to encompass more of the compute heavy tasks in a variety of problem domains. The NPP library is written to maximize flexibility, while maintaining high performance.\n\nnvJitLinkThe user guide for the nvJitLink library.\n\nnvFatbinThe user guide for the nvFatbin library.\n\nNVRTC (Runtime Compilation)NVRTC is a runtime compilation library for CUDA C++. It accepts CUDA C++ source code in character string form and creates handles that can be used to obtain the PTX. The PTX string generated by NVRTC can be loaded by cuModuleLoadData and cuModuleLoadDataEx, and linked with other modules by cuLinkAddData of the CUDA Driver API. This facility can often provide optimizations and performance not possible in a purely offline static compilation.\n\nThrustThe C++ parallel algorithms library.\n\ncuSOLVERThe cuSOLVER library user guide.\n\n\n\n\n\nPTX Compiler API References\uf0c1\n\nPTX Compiler APIsThis guide shows how to compile a PTX program into GPU assembly code using APIs provided by the static PTX Compiler library.\n\n\n\n\n\n\nMiscellaneous\uf0c1\nCUDA Demo SuiteThis document describes the demo applications shipped with the CUDA Demo Suite.\n\nCUDA on WSLThis guide is intended to help users get started with using NVIDIA CUDA on Windows Subsystem for Linux (WSL 2). The guide covers installation and running CUDA applications and containers in this environment.\n\nMulti-Instance GPU (MIG)This edition of the user guide describes the Multi-Instance GPU feature of the NVIDIA\u00ae A100 GPU.\n\nCUDA CompatibilityThis document describes CUDA Compatibility, including CUDA Enhanced Compatibility and CUDA Forward Compatible Upgrade.\n\nCUPTIThe CUPTI-API. The CUDA Profiling Tools Interface (CUPTI) enables the creation of profiling and tracing tools that target CUDA applications.\n\nDebugger APIThe CUDA debugger API.\n\nGPUDirect RDMAA technology introduced in Kepler-class GPUs and CUDA 5.0, enabling a direct path for communication between the GPU and a third-party peer device on the PCI Express bus when the devices share the same upstream root complex using standard features of PCI Express. This document introduces the technology and describes the steps necessary to enable a GPUDirect RDMA connection to NVIDIA GPUs within the Linux device driver model.\n\nGPUDirect StorageThe documentation for GPUDirect Storage.\n\nvGPUvGPUs that support CUDA.\n\n\n\n\n\nTools\uf0c1\n\nNVCCThis is a reference document for nvcc, the CUDA compiler driver. nvcc accepts a range of conventional compiler options, such as for defining macros and include/library paths, and for steering the compilation process.\n\nCUDA-GDBThe NVIDIA tool for debugging CUDA applications running on Linux and QNX, providing developers with a mechanism for debugging CUDA applications running on actual hardware. CUDA-GDB is an extension to the x86-64 port of GDB, the GNU Project debugger.\n\nCompute SanitizerThe user guide for Compute Sanitizer.\n\nNsight Eclipse Plugins Installation GuideNsight Eclipse Plugins Installation Guide\n\nNsight Eclipse Plugins EditionNsight Eclipse Plugins Edition getting started guide\n\nNsight SystemsThe documentation for Nsight Systems.\n\nNsight ComputeThe NVIDIA Nsight Compute is the next-generation interactive kernel profiler for CUDA applications. It provides detailed performance metrics and API debugging via a user interface and command line tool.\n\nNsight Visual Studio EditionThe documentation for Nsight Visual Studio Edition.\n\nProfilerThis is the guide to the Profiler.\n\nCUDA Binary UtilitiesThe application notes for cuobjdump, nvdisasm, and nvprune.\n\n\n\n\n\nWhite Papers\uf0c1\n\nFloating Point and IEEE 754A number of issues related to floating point accuracy and compliance are a frequent source of confusion on both CPUs and GPUs. The purpose of this white paper is to discuss the most common issues related to NVIDIA GPUs and to supplement the documentation in the CUDA C++ Programming Guide.\n\nIncomplete-LU and Cholesky Preconditioned Iterative MethodsIn this white paper we show how to use the cuSPARSE and cuBLAS libraries to achieve a 2x speedup over CPU in the incomplete-LU and Cholesky preconditioned iterative methods. We focus on the Bi-Conjugate Gradient Stabilized and Conjugate Gradient iterative methods, that can be used to solve large sparse nonsymmetric and symmetric positive definite linear systems, respectively. Also, we comment on the parallel sparse triangular solve, which is an essential building block in these algorithms.\n\n\n\n\n\nApplication Notes\uf0c1\n\nCUDA for TegraThis application note provides an overview of NVIDIA\u00ae Tegra\u00ae memory architecture and considerations for porting code from a discrete GPU (dGPU) attached to an x86 system to the Tegra\u00ae integrated GPU (iGPU). It also discusses EGL interoperability.\n\n\n\n\n\nCompiler SDK\uf0c1\n\nlibNVVM APIThe libNVVM API.\n\nlibdevice User\u2019s GuideThe libdevice library is an LLVM bitcode library that implements common functions for GPU kernels.\n\nNVVM IRNVVM IR is a compiler IR (intermediate representation) based on the LLVM IR. The NVVM IR is designed to represent GPU compute kernels (for example, CUDA kernels). High-level language front-ends, like the CUDA C compiler front-end, can generate NVVM IR.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrivacy Policy\r\n|\r\nManage My Privacy\r\n|\r\nDo Not Sell or Share My Data\r\n|\r\nTerms of Service\r\n|\r\nAccessibility\r\n|\r\nCorporate Policies\r\n|\r\nProduct Security\r\n|\r\nContact\n\n\u00a9 Copyright 2007-2024, NVIDIA Corporation & affiliates. All rights reserved.\r\n      Last updated on Jul 1, 2024.\r\n      \n\n\n\n\n\n\n\n\n"}, {"url": "https://developer.nvidia.com/nvidia-video-codec-sdk", "content": "\n\n\n\n\n\n\nVideo Codec SDK | NVIDIA Developer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNVIDIA Video Codec SDK\nA comprehensive set of APIs including high-performance tools, samples and documentation for hardware accelerated video encode and decode on Windows and Linux.\n\n\r\n\t\t  Get Started\r\n\t\t\n\n\n\n\n\n\nNVIDIA GeForce Now is made possible by leveraging NVENC in the datacenter and streaming the result to end clients\r\n\t\t\n\n\n\n\n\nHardware Based Decoder and Encoder\n\n\nNVIDIA GPUs contain one or more hardware-based decoder and encoder(s) (separate from the CUDA cores) which provides fully-accelerated hardware-based video decoding and encoding for several popular codecs. With decoding/encoding offloaded, the graphics engine and the CPU are free for other operations.\nGPU hardware accelerator engines for video decoding (referred to as NVDEC) and video encoding (referred to as NVENC) support faster than real-time video processing which makes them suitable to be used for transcoding applications, in addition to video playback. Video Codec SDK lets you harness NVENC and NVDEC for real-time 8k 60FPS AV1 and HEVC video on Ada Lovelace architecture.\n\n\n\n\n\n\n\n\n\n\nNVENC - Hardware-Accelerated Video Encoding\nIntroducing AV1 encoding with Video Codec SDK 12.0 on NVIDIA\u2019s Ada architecture. AV1 is the state of the art video coding format that supports higher quality with better performance compared to H.264 and HEVC. On Ada, multiple NVENC coupled with AV1 enables encoding 8k video at 60fps alongside a higher number of concurrent sessions. With complete encoding (which is computationally complex) offloaded to NVENC, the graphics engine and the CPU are free for other operations. For example, in a game recording and streaming scenario like streaming to Twitch.tv using Open Broadcaster Software (OBS), encoding being completely offloaded to NVENC makes the graphics engine bandwidth fully available for game rendering.\nNVENC enables streaming applications at high quality and ultra-low latency without utilizing the CPU, encoding at very high quality for archiving, OTT streaming, web videos, and encoding with ultra-low power consumption per stream (Watts/stream)\n\n\n\n\n\nNote: These graphs showcases performance on NVIDIA datacenter T4,  A10 and L40.\nBitrate savings are BD-BR based on PSNR, average across a large variety of content (several hundreds of video clips), using FFmpeg.\nOnly datacenter GPUs are presented on the benchmark graphs for clarity but equivalent workstation GPU with same architecture performs similarly\u200b. To learn more about the hardware details, the process and software configuration used for generating above data, please refer to this detailed documentation\u200b.\n\n\n\n\nSupported Format Details (Click to learn more)\n\n\n\n\n\n\n\nGPU\nH.264 (AVCHD) YUV\u00a04:2:0\nH.264 (AVCHD) YUV\u00a04:4:4\nH.264 (AVCHD) LOSSLESS\nH.265 (HEVC) YUV\u00a04:2:0\nH.265 (HEVC) YUV\u00a04:4:4\nH.265 (HEVC) LOSSLESS\nAV1\n\n\nMAX Color\nMAX Res.\nMAX Color\nMAX Res.\nMAX Color\nMAX Res.\nMAX Color\nMAX Res.\nMAX Color\nMAX Res.\nMAX Color\nMAX Res.\nMAX Color\nMAX Res.\n\n\n\n\nMaxwell (1st Gen)*\n8-bit\n4096 x 4096\n8-bit\n4096 x 4096\n8-bit\n4096 x 4096\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\n\n\nMaxwell (2nd Gen)\n8-bit\n4096 x 4096\n8-bit\n4096 x 4096\n8-bit\n4096 x 4096\n8-bit\n4096 x 4096\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\n\n\nMaxwell (GM206)\n8-bit\n4096 x 4096\n8-bit\n4096 x 4096\n8-bit\n4096 x 4096\n8-bit\n4096 x 4096\n8-bit\n4096 x 4096\n8-bit\n4096 x 4096\nN/A\nN/A\n\n\nPascal\n8-bit\n4096 x 4096\n8-bit\n4096 x 4096\n8-bit\n4096 x 4096\n10-bit\n8192 x 8192**\n10-bit\n8192 x 8192**\n10-bit\n8192 x 8192**\nN/A\nN/A\n\n\nVolta\n8-bit\n4096 x 4096\n8-bit\n4096 x 4096\n8-bit\n4096 x 4096\n10-bit\n8192 x 8192\n10-bit\n8192 x 8192\n10-bit\n8192 x 8192\nN/A\nN/A\n\n\nTuring\n8-bit\n4096 x 4096\n8-bit\n4096 x 4096\n8-bit\n4096 x 4096\n10-bit\n8192 x 8192\n10-bit\n8192 x 8192\n10-bit\n8192 x 8192\nN/A\nN/A\n\n\nAmpere(A100)\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\n\n\nAmpere(non A100)\n8-bit\n4096 x 4096\n8-bit\n4096 x 4096\n8-bit\n4096 x 4096\n10-bit\n8192 x 8192\n10-bit\n8192 x 8192\n10-bit\n8192 x 8192\nN/A\nN/A\n\n\nAda\n8-bit\n4096 x 4096\n8-bit\n4096 x 4096\n8-bit\n4096 x 4096\n10-bit\n8192 x 8192\n10-bit\n8192 x 8192\n10-bit\n8192 x 8192\n10-bit\n8192 x 8192\n\n\n\n* Except GM108 and GP108 (not supported)\n** Except GP100 (is limited to 4K resolution)\n\n\n\n\n\n\n\n\nNVDEC - Hardware-Accelerated Video Decoding\nNVIDIA GPUs contain a hardware-based decoder (referred to as NVDEC) which provides fully-accelerated hardware-based video decoding for several popular codecs. With complete decoding offloaded to NVDEC the graphics engine and the CPU are free for other operations. NVDEC supports much faster than real-time decoding which makes it suitable to be used for transcoding applications, in addition to video playback applications.\nNVDECODE API enables software developers to configure this dedicated hardware video decoder. This dedicated accelerator supports hardware-accelerated decoding of the following video codecs on Windows and Linux platforms: MPEG-2, VC-1, H.264 (AVCHD), H.265 (HEVC), VP8, VP9 and AV1 (see table below for codec support for each GPU generation).\n\n\n\n\n\n\nSupported Format Details (Click to learn more)\n\n\n\n\n\n\n\nGPU\n*H.265\u00a0(HEVC)\u00a04:4:4\nH.265\u00a0(HEVC)\u00a04:2:0\nH.264\u00a0(AVCHD)\u00a04:2:0\nVP9\nVP8\nMPEG-2\nVC-1\nAV1\n\n\nMAX Color\nMAX Res.\nMAX Color\nMAX Res.\nMAX Color\nMAX Res.\nMAX Color\nMAX Res.\nMAX Color\nMAX Res.\nMAX Color\nMAX Res.\nMAX Color\nMAX Res.\nMAX Color\nMAX Res.\n\n\n\n\nKepler\nN/A\nN/A\nN/A\nN/A\n8-bit\n4096 x 4096\nN/A\nN/A\nN/A\nN/A\n8-bit\n4080 x 4080\n8-bit\n2048 x 1024\nN/A\nN/A\n\n\nMaxwell (1st Gen)*\nN/A\nN/A\nN/A\nN/A\n8-bit\n4096 x 4096\nN/A\nN/A\nN/A\nN/A\n8-bit\n4080 x 4080\n8-bit\n2048 x 1024\nN/A\nN/A\n\n\nMaxwell (2nd Gen)\nN/A\nN/A\nN/A\nN/A\n8-bit\n4096 x 4096\nN/A\nN/A\n8-bit\n4096 x 4096\n8-bit\n4080 x 4080\n8-bit\n2048 x 1024\nN/A\nN/A\n\n\nMaxwell (GM206)\nN/A\nN/A\n10-bit\n4096 x 2304\n8-bit\n4096 x 4096\n8-bit\n4096 x 2304\n8-bit\n4096 x 4096\n8-bit\n4080 x 4080\n8-bit\n2048 x 1024\nN/A\nN/A\n\n\nPascal\nN/A\nN/A\n12-bit\n8192 x 8192**\n8-bit\n4096 x 4096\n12-bit****\n8192 x 8192**\n8-bit\n4096 x 4096***\n8-bit\n4080 x 4080\n8-bit\n2048 x 1024\nN/A\nN/A\n\n\nVolta\nN/A\nN/A\n12-bit\n8192 x 8192\n8-bit\n4096 x 4096\n12-bit\n8192 x 8192\n8-bit\n4096 x 4096\n8-bit\n4080 x 4080\n8-bit\n2048 x 1024\nN/A\nN/A\n\n\nTuring\n12-bit\n8192 x 8192\n12-bit\n8192 x 8192\n8-bit\n4096 x 4096\n12-bit\n8192 x 8192\n8-bit\n4096 x 4096\n8-bit\n4080 x 4080\n8-bit\n2048 x 1024\nN/A\nN/A\n\n\nAmpere(A100)\n12-bit\n8192 x 8192\n12-bit\n8192 x 8192\n8-bit\n4096 x 4096\n12-bit\n8192 x 8192\n8-bit\n4096 x 4096\n8-bit\n4080 x 4080\n8-bit\n2048 x 1024\nN/A\nN/A\n\n\nAmpere(non A100)\n12-bit\n8192 x 8192\n12-bit\n8192 x 8192\n8-bit\n4096 x 4096\n12-bit\n8192 x 8192\n8-bit\n4096 x 4096\n8-bit\n4080 x 4080\n8-bit\n2048 x 1024\n10-bit\n8192 x 8192\n\n\nAda\n12-bit\n8192 x 8192\n12-bit\n8192 x 8192\n8-bit\n4096 x 4096\n12-bit\n8192 x 8192\n8-bit\n4096 x 4096\n8-bit\n4080 x 4080\n8-bit\n2048 x 1024\n10-bit\n8192 x 8192\n\n\n\n* Except GM108 (not supported)\n** Max resolution support is limited to selected Pascal chips\n*** VP8 decode support is limited to selected Pascal chips\n**** VP9 10/12 bit decode support is limited to select Pascal chips\n\n\n\n\n\n\nVideo Codec APIs at NVIDIA\nNVIDIA has provided hardware-accelerated video processing on GPUs for over a decade through the NVIDIA Video Codec SDK. This is a comprehensive set of APIs, high-performance tools, samples, and documentation for hardware-accelerated video encode and decode on Windows and Linux. \r\n\t  \nNVIDIA also supports GPU-accelerated encode and decode through Microsoft\u2019s DirectX Video, a cross-vendor API for Windows developers and Vulkan Video which has both Linux and Windows support. In contrast to the NVIDIA Video Codec SDK, both DirectX Video and Vulkan Video are low-level APIs. While the Video Codec SDK provides automation for C++ developers, DirectX Video and Vulkan Video provide precise control over video streaming through hardware acceleration blocks, empowering applications to efficiently orchestrate system resources. \r\n\t  \nWhether you prefer DirectX or Vulkan, you can combine flexible GPU-accelerated video encoding and decoding with other GPU acceleration, like 3D and AI, using the language of your choice.\r\n\t  \nThe low-level Vulkan Video extensions are also attractive to developers of popular open-source streaming media frameworks such as GStreamer and FFmpeg both of which are being actively ported to Vulkan Video. The cross-platform availability of Vulkan will enable accelerated GPU processing for these frameworks across multiple platforms without needing to port to multiple proprietary video APIs. Please refer to the Vulkan Video getting started page for more details.\nPyNvVideoCodec is another set of APIs introduced in Q4 2023, which provides simple APIs for harnessing video encoding and decoding capabilities when working with videos in Python. PyNvVideoCodec is a library that provides python bindings over C++ APIs for hardware accelerated video encoding and decoding.\nVideo Codec SDK, DirectX Video, Vulkan Video and PyNvVideoCodec provide complementary support to GPU-accelerated video workflows. NVIDIA will continue to support all listed APIs providing developers with the option to use the ones that best suit their needs.\n\n\n\n\n\nVulkan Video\nDirectX Video\nNVIDIA Video Codec SDK\nPyNvVideoCodec\n\n\nPlatform\nWindows and Linux\r\n\t\t\t\n\nWindows\r\n\t\t\t\n\nWindows and Linux\r\n\t\t\t\n\nWindows and Linux\r\n\t\t\t\n\n\n\nBenefits\n\n\nLow Level Control\nNative Vulkan Integration\nEasy for Vulkan developers\nMulti Vendor\n\n\n\n\n\nLow Level Control\nNative DirectX and Windows Integration\nEasy for DirectX developers\nMulti-Vendor\n\n\n\n\n\nHigh Level Control\nNative Integration in custom pipelines\nUseful for users with less knowledge of Vulkan and Direct X\nEasy for C, C++ developers\nNvidia Proprietary API\nComprehensive feature set\n\n\n\n\n\nPython bindings over C++ Video Codec SDK wrapper classes\nEasy for Python developers\nNvidia Proprietary API\n\n\n\n\n\nNative API interface\nVulkan Graphics\r\n\t\t\t\n\nD3D11 (Decode only) and D3D12\r\n\t\t\t\n\nD3D9, D3D10, D3D11, D3D12 (Encode only)\r\n\t\t\t  CUDA (Encode and decode)            \r\n\t\t\t\n\nCUDA (Encode and decode)\r\n\t\t\t\n\n\n\n\n\n\n\n\nPartners and Examples\n\n\n\n\n\n\nPremiere Pro is the industry-leading video editing\r\n\t\t\tapplication for film, TV, social, and online content.\r\n\t\t\t\r\n\t\t\t  Learn\r\n\t\t\t  More\r\n\t\t\t\n\n\n\n\n\n\n\n\n\n\n\nBlackmagic is a leading manufacturer of creative video technology. Dedicated to quality and stability; Blackmagic is world famous for their codecs and affordable high-end quality editing workstations built upon Blackmagic software and hardware.\n\nVisit\r\n\t\t\t\t  Blackmagic for detailed product information\n\n\n\n\n\n\n\n\n\n\nComprimato is a JPEG2000 software codec\r\n\t\t\t  toolkit offering media & entertainment and geospatial imaging\r\n\t\t\t  technology company\u2019s life-like viewing experience that result in\r\n\t\t\t  better enjoyment and more accurate decision-making. The JPEG2000\r\n\t\t\t  standard compliant Ultra HD software codec leverages the\r\n\t\t\t  supercomputing power of GPUs and CPUs to speed up video and image\r\n\t\t\t  compression by 10x. The codec saves infrastructure costs by 70%,\r\n\t\t\t  reducing development cycles by 50% and enabling new revenue streams\r\n\t\t\t  such as Ultra HD, High Dynamic Range (HDR) and High Frame Rate (HFR)\r\n\t\t\t  video.\nVisit Comprimato\r\n\t\t\t\tfor detailed product information\n\n\n\n\n\n\n\n\n\n\n\nDELTACAST develops state-of-the-art products for the professional TV broadcast market, providing a range of cost-effective video cards that, with the SDK software, can be used in OEM products to create professional broadcast custom solutions and products.\nVisit DELTACAST for detailed product information\n\n\n\n\n\n\n\n\n\n\n\nErlyvideo LLC has been developing software for streaming video since 2010. Our carrier-grade server solutions help business clients capture, process, transcode, archive, and deliver video to millions of subscribers. We are making every effort to research and develop reliable, premium quality products that truly meet customer needs. Customers in more than 100 countries use our products for building IPTV/OTT, CDN, and Video Surveillance as a Service CCTV systems.\nFlussonic Media Server is a multi-purpose software solution for launching high load video streaming services. Using Flussonic Media Server you can set up an end-to-end video streaming pipeline of any scale. Flussonic can ingress and egress videos in almost any format, codec, and resolution. It will process and transcode incoming streams and deliver beautiful video to your subscribers. Let your business benefit from the most advanced and efficient video streaming platform.\nVisit Flussonic.com for detailed product information\n\n\n\n\n\n\n\n\n\nGcore  accelerates AI training, provides comprehensive cloud services, improves content delivery, and protects servers and applications.\nVisit Gcore for detailed product information ((https://gcore.com/)\n\n\n\n\n\n\n\n\n\nFastvideo is a world leader in the field of high performance GPU-based image and video processing. Fastvideo team consists of experienced and highly dedicated professionals and it focuses on GPU image processing, algorithm design and parallel computations. Our technologies show unmatched performance in image compression and decompression (JPEG, JPEG2000, Raw Bayer), demosaicing, denoising, tone mapping, color correction, resizing, sharpening, encoding and decoding of video streams in various applications including image and video processing, high speed imaging, machine vision and other camera applications, streaming, digital cinema, 3D and VR, broadcasting, etc.\n\n\n\nVisit Fastvideo for detailed product information\n\n\n\n\n\n\n\n\n\nMainConcept has been the premier provider of video and audio codecs, plugins and applications to the production, streaming and broadcast industries for three decades. As the technology of choice for some of the most valued brands across the globe, MainConcept supports robust video workflows from ingest through delivery.\nThe MainConcept\u00ae Hybrid GPU HEVC Encoder combines the market-leading MainConcept\u00ae HEVC software encoder with the unrivaled performance of NVIDIA RTX architecture, bridging the gap between high-quality software and fast-performing hardware encoding. Leveraging MainConcept\u2019s reliable, market-proven algorithms for rate control and quality encoding with the processing power of NVIDIA, the MainConcept Hybrid GPU HEVC Encoder gives you best-in-class image quality (up to 8K) at tremendous speed.\n\n\n\n\n\n\"Enabling access to HEVC/H.265 video encoding in hardware allows our customers to continue working in the well-known MainConcept environment with its rich portfolio of multiplexers and auxiliary components, while benefiting from the computational power of NVIDIA GPUs. Using NVIDIA GPUs for HEVC/H.265 encoding increases server density for processing multiple video streams on one system while still having enough CPU cycles available for applications.\" Deacon Johnson, SVP Global Sales - Technology Licensing for MainConcept\nLearn more about MainConcept Hybrid GPU HEVC Encoder\n\n\n\n\n\n\n\n\n\nMedialooks, founded in 2005, provides broadcast customers with a high-level software development kit to quickly build playout automation, virtual studio and video capture solutions. Customers include PlayBox Technologies, Arvato Systems, Masterplay, Winjay, Etere, Axel Technology, Xeus Media, Wolftech and Broadcast Play.\nVisit Medialooks for detailed product information\n\n\n\n\n\n\n\n\n\nMulticamera.Systems LLC is a developer of video acquisition and recording software for machine vision cameras, catering for variety of industries: science labs, VR, sport analytics autonomous cars and military. \"The Recorder\" software is the only software on the market capable of recording h.26x compressed video at thousands of frames per second thanks to our own \"GPUSqueeze\" library supporting multi-GPU video compression. This library is now available for third party developers.\n\"The Recorder\" software major features:\n\n\n\r\n\t\t\t\t  unparalleled support for multi-camera and multi-PC setups\n\n\r\n\t\t\t\t  compatible with all machine vision camera types: USB-Vision,\r\n\t\t\t\t  GigE-Vision, CoaXPress and CLHS\r\n\t\t\t\t\n\n\r\n\t\t\t\t  full remote control of recording system with real time video\r\n\t\t\t\t  streaming\r\n\t\t\t\t\n\n\r\n\t\t\t\t  recording to Uncompressed, M-JPEG and H.26x formats at any\r\n\t\t\t\t  frame rates\r\n\t\t\t\t\n\n\r\n\t\t\t\t  easy integration with user\u2019s own modules for online and\r\n\t\t\t\t  offline image processing\r\n\t\t\t\t\n\n\"GPUSqueeze\" library major features:\n\n\n\r\n\t\t\t\t  can be easily integrated into user\u2019s own application to enable high speed and multi-stream video encoding and transcoding\n\n\r\n\t\t\t\t  the library can accept bayer images and performs high quality demosaicing\n\n\r\n\t\t\t\t  all image processing is done purely on GPU and performance scales linearly with the increase of number of GPUs in a system\n\n\r\n\t\t\t\t  the library supports mixed GPU configuration (e.g. GTX 1660 + RTX 2080) allowing extra flexibility and cost saving to end users\n\n\n\n\n\nNVIDIA's high performance Video Engine alongside with CUDA enabled us to develop \u201cGPUSqueeze\u201d library and opened up the possibility of long high speed video recordings for the users of \u201cThe Recorder\u201d software and can greatly increate performance of video encoding or transcoding applications being developed by users. Dmitry Semiannikov, Founder and Director, Multicamera.Systems LLC\n\nVisit Multicamera.systems for more information on the \"GPUSqueeze\" library Visit Medialooks for detailed product information on \"The Recorder\" software\n\n\n\n\n\n\n\n\n\n\n\nNorpix is a developer of digital video recording software for scientific, machine vision, military and general purpose digital video acquisition applications. We market the industries number 1 DVR software Streampix for single or multiple camera acquisition. We also develop an SDK and CUDA JPEG compression library that runs on NVIDIA GPU\u2019s.\nBenefit of the product:\n\n\n\r\n\t\t\t\t  Capture in real time lossy JPEG with a quality factor varying from 1 to 99%.\n\n\r\n\t\t\t\t  Compress up to 3.0 billion pixels in monochrome.\n\n\r\n\t\t\t\t  Up to 2.2 billion pixels in color.\n\n\r\n\t\t\t\t  Compatible usb3Vision, GigE Vision, 10 GigE Vision, Camera Link and CoaXPress cameras.\n\n\r\n\t\t\t\t  Supports multiple nVidia GPU for parallel processing load balancing over multiple cameras.\n\n\r\n\t\t\t\t  Stand along C/C++ libraries and run time.\n\nVisit Norpix for detailed product information\n\n\n\n\n\n\n\n\n\n\n\nNVIDIA GeForce NOW\u2122 is an on-demand service that connects you to NVIDIA\u2019s cloud-gaming supercomputers to stream PC games to your SHIELD device at up to 1080p resolution and 60 frames per second.\nLearn more about GeForce NOW\n\n\n\n\n\n\n\n\n\n\n\nOBS Studio is a free and open source software designed for capturing, compositing, encoding, recording, and live streaming video content, efficiently.\nLearn more about OBS Studio\nBLOG - New GeForce-Optimized OBS and RTX Encoder Enables Pro-Quality Broadcasting on a Single PC\n\n\n\n\n\n\n\n\n\nBased in Hong Kong and with a development center in Manila, Philippines. More than 8 Million people and businesses use SplitmediaLabs products to grow their communities, create innovative content and connect with other players from around the world. SplitmediaLabs has helped usher in the new age of live streaming gameplay since its creation back in 2009 and is the developer XSplit, Challonge and Player.me.\nXSplit Broadcaster: A simple yet powerful live streaming software and recording software that powers countless live streams and recordings around the world. XSplit Broadcaster is the perfect solution for producing and delivering rich video content.\n With NVIDIA NVENC, single PC game streaming with XSplit has never been easier. By natively integrating the NVIDIA Video Codec SDK, XSplit is able to offload the video encoding for both the recording and the live stream from the CPU to the GPU, allowing our users to produce high quality content without compromising on gaming performance. Miguel Molina, Director of Developer Relations, SplitmediaLabs Limited\n\nLearn more about XSplit\n\n\n\n\n\n\n\n\n\nStreamline is a reference system design for a premium quality, white label, end to end live streaming system from HDMI / HD-SDI capture all the way to a player on a CDN that works on web, iOS, and Android devices. Using commodity computer hardware, free software, and AWS, it\u2019s an affordable way to learn how to build a very high quality live streaming system.\nLearn more about Streamline\n\n\n\n\n\n\n\n\n\nTelestream\u00ae specializes in products that make it possible to get video content to any audience regardless of how it is created, distributed or viewed. Throughout the entire digital media lifecycle, from capture to viewing, for consumers through high-end professionals, Telestream products range from desktop components and cross-platform applications to fully-automated, enterprise-class digital media transcoding and workflow systems. Telestream enables users in a broad range of business environments to leverage the value of their video content.\nVisit Telestream for detailed product information\n\n\n\n\n\n\n\n\n\nWowza Media Systems\u2122 is the recognized gold standard of streaming, with more than 22,000 customers in 170+ countries. By reducing the complexities of video and audio delivery to any device, Wowza\u2122 enables organizations to expand their reach and more deeply engage their audiences, in industries ranging from education to broadcasting. Service providers, direct customers and partners worldwide trust Wowza products to provide robust, customizable and scalable streaming solutions\u2014with powerful APIs and SDKs to meet organizations\u2019 evolving streaming needs. Wowza was founded in 2005, is privately held, and is headquartered in Colorado.\nVisit Wowza for detailed product information\n\n\n\n\n\n\n\n\n\n\n\nBeamr (NASDAQ:BMR) is a video technology and image science software company. Beamr is a leading provider of video encoding, transcoding, and optimization software solutions that enable high quality, performance, and bitrate efficiency for Live and VOD video services. \r\n\t\t\r\n\t\t\r\n\t\t\r\n\t\tSince 2021 Beamr and Nvidia have been working together to port Beamr's Content Adaptive BitRate (CABR) and seamlessly integrate it with Nvidia's NVENC video encoder to create an accelerated video optimization solution. This guarantees the highest video quality at the lowest bitrate possible and is available to all NVENC encoders.\r\n\t\t\r\n\t\t\r\n\t\t\r\n\t\tIn Oct 2023 Beamr launched a cloud service running over Nvidia's NVENC with the vision of democratizing videooptimization at large scale.\r\n\t\t\r\n\t\t\nLearn more about Beamr\n\n\n\n\n\n\n\n\n\nVideo Codec SDK in the News\n\n\n\n\n\nNew Video Creation and Streaming Features Accelerated by the NVIDIA Video Codec SDK\nVideo Codec SDK 12.1 is available now, bringing improvements to split encoding and a new low-level NVENC API. Learn about the new features and how they have been used to accelerate video creation and streaming. \n\r\n\t\t\t\tRead more \n\n\n\n\n\n\n\n\nImproving Video Quality and Performance with AV1 and NVIDIA Ada Lovelace Architecture\nAV1 is the new gold standard video format, with superior efficiency and quality compared to older H.264 and H.265 formats. It is the most recent royalty-free, efficient video encoder standardized by the Alliance for Open Media. \n\r\n\t\t\t\tRead more \n\n\n\n\n\n\n\n\nAV1 Encoding and FRUC: Video Performance Boosts and Higher Fidelity on the NVIDIA Ada Architecture\nUpdates to Video Codec SDK, including AV1 encoding on the new Ada GPU generation and updates to Optical Flow SDK, including the new frame rate up conversion library, are announced at GTC.\n\n\r\n\t\t\t\tRead more \n\n\n\n\n\n\n\n\nNVIDIA Video Technologies in Ada\nNVIDIA GPUs contain dedicated hardware for video encoding, decoding, JPEG sill image decoding and optical flow computation. This talk covers latest features supported by Ada GPUs as well as software updates such as new SDK features, use-cases etc.\r\n\t\t\t\t\n\r\n\t\t\t\t  Watch \n\n\n\n\n\n\n\n\n\n\nCut to the Video: Adobe Premiere Pro Helps Content Creators Work Faster with GPU-Accelerated Exports\nWith NVIDIA encoder acceleration in Adobe Premiere Pro, editors can export high-resolution videos up to 5x faster than on CPU.\n\r\n\t\t\t\tRead more \n\n\n\n\n\n\n\n\nVideo Codec SDK Connect with Experts Series\nQ&A style sessions  provide an overview of the  two SDKs including new features and enhancements, provide tips for efficient use, and address any open questions from developers.\n\r\n\t\t\t\tWatch \n\n\n\n\n\n\n\n\n\nResources\n\n\n\n\n\n\n\n\nGPU Support Matrix\n\n\n\n\n\n\n\n\n\n\nGet Started Developing with Video Codec SDK.\nGet Started\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}, {"url": "https://nvlabs.github.io/cub/", "content": "\n\nRedirecting to nvidia.github.io/cccl/cub\n\n\n\n\n\n"}, {"url": "https://nvidia.github.io/libcudacxx/", "content": "\n\nRedirecting to nvidia.github.io/cccl/libcudacxx\n\n\n\n\n"}, {"url": "https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html", "content": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncuFile API Reference Guide - NVIDIA Docs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubmit Search\n\n\n\n\n\nNVIDIA Developer\n\n\n\n\n\nBlog\n\n\n\n\n\nForums\n\n\n\n\n\nJoin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubmit Search\n\n\n\n\n\n\n\nNVIDIA Developer\n\n\n\n\n\nBlog\n\n\n\n\n\nForums\n\n\n\n\n\nJoin\n\n\n\n\n\n\nMenu\n\n\n\n\n\n\n\n\n\n\n\n\n\ncuFile API Reference Guide\n\n\n\n\n\n\n\nSubmit Search\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubmit Search\n\n\n\nNVIDIA Docs Hub\u00a0\u00a0NVIDIA GPUDirect Storage (GDS)\u00a0\u00a0NVIDIA GPUDirect Storage\u00a0\u00a0cuFile API Reference Guide\n\n\nNVIDIA GPUDirect Storage (GDS) (Latest Release)\nDownload PDF\n\n\n\n\n\n \n\nGDS cuFile API Reference\n\n \n       \n       The NVIDIA\u00ae GPUDirect\u00ae Storage cuFile API Reference Guide provides information about the cuFile API reference that is used in applications and frameworks to leverage GDS technology and describes the intent, context, and operation of those APIs, which are part of the GDS technology. \n\n\n\n\n1. Introduction\n\n\n \n       \n       \n       NVIDIA\u00ae Magnum IO GPUDirect\u00ae Storage (GDS) is part of the GPUDirect family. GDS enables a direct data path for direct memory access (DMA) transfers between GPU memory and storage, which avoids a bounce buffer through the CPU. This direct path increases system bandwidth and decreases the latency and utilization load on the CPU. \n\nThis document provides information about the cuFile APIs that are used in applications and frameworks to leverage GDS technology and describes the intent, context, and operation of those APIs which are part of the GDS technology.  \n        \n        \n\n Note:\n\n          The APIs and descriptions are subject to change without notice. \n        \n\n\n\n\n\n\n2. Usage\n\n\n \n       \n       \n       This section describes the operation of the cuFile APIs.\nBecause the functionality is part of the CUDA Driver C API, the APIs use the cuFile prefix and camel case motif of the CUDA Driver.  \n       \nAll APIs are thread-safe.\nThe fork system call should not be used after the library is initialized. The behavior of the APIs after the fork system call is undefined in the child process. \nThe APIs with GPU buffers should be called in a valid CUDA context and stream if applicable. \nAll APIs are issued from the CPU, not the GPU.\n\n\n\n\n Note:\n\n          Starting from CUDA toolkit 12.2 (GDS version 1.7.x) release cuFile APIs support memory allocated on GPU device as well as host memory. peer to peer transfer using GPUDirect\u2122 is supported to and from device memory on supported file system and hardware configurations. The APIs will refer to this memory address as buffer pointer unless the API specifically applies to a particular type of memory. \n        \n\n\n \n\n\n\n2.1.\u00a0Dynamic Interactions\n\n\n \n        \n        The following describes the dynamic interactions between the cuFile APIs.\nSome of the cuFile APIs are optional. If they are not called proactively, their actions will occur reactively:  \n        If cuFile{DriverOpen, HandleRegister, BufRegister} is called on a driver, file, or buffer, respectively that has been opened or registered by a previous cuFile* API call, this will result in an error. Calling cuFile{BufDeregister, HandleDeregister, DriverClose} on a buffer, file, or driver, respectively that has never been opened or registered by a previous cuFile* API call results in an error. For these errors, the output parameters of the APIs are left in an undefined state, and there are no other side effects.  \n        \n  cuFileDriverOpen explicitly causes driver initialization.  Its use is optional. If it is not used, driver initialization happens implicitly at the first use of the cuFile{HandleRegister, Read, Write, BufRegister} APIs.  \n\n\n(Mandatory) cuFileHandleRegister turns an OS-specific file descriptor into a CUfileHandle_t and performs checking on the GDS supportability based on the mount point and the way that the file was opened. \n  cuFileBufRegister explicitly registers a memory buffer.  If this API is not called, an internal registered memory is used if required on the first time the buffer is used, for example, in cuFile{Read, Write}.  \n  cuFile{BufDeregister, HandleDeregister} explicitly frees a buffer and file resources, respectively.  If this API is not called, the buffer and resources are implicitly freed when the driver is closed using cuFileDriverClose.  \n  cuFileDriverClose explicitly frees driver resources.  If this API is not called, the driver resources are implicitly freed when dlclose() is performed on the library handle or when the process is terminated.  \n\n \n\n\n\n2.2.\u00a0Driver, File, and Buffer Management\n\n\n \n        \n        This section describes the overall workflow to manage the driver, the file, and buffer management: \n\nCall cuFileDriverOpen() to initialize the state of the critical performance path. \nAllocate GPU memory with cudaMalloc, cudaMallocManaged, cuMem* APIs or host memory using cudaMallocHost, malloc or mmap. \nTo register the buffer, call cuFileBufRegister to initialize the buffer state of the critical performance path. \nComplete the following IO workflow: \n  \nFor Linux, open a file with POSIX open.\nCall cuFileHandleRegister to wrap an existing file descriptor in an OS-agnostic CUfileHandle_t. This step evaluates the suitability of the file state and the file mount for GDS and initializes the file state of the critical performance path. \nCall IO APIs such as cuFileRead/cuFileWrite on an existing cuFile handle and existing buffer. \n    \nIf the cuFileBufRegister has not been previously called on the buffer pointer, cuFileRead/cuFileWrite will use internal registered buffers when required. \n Not using cuFileBufRegister might not be performant for small IO sizes.  \n Refer to the GPUDirect Best Practices Guide for more information.  \n \nUnless an error condition is returned, the IO is performed successfully. \n \nCall cuFileBufDeregister to free the buffer-specific cuFile state. \nCall cuFileHandleDeregister to free the file-specific cuFile state. \nCall cuFileDriverClose to free up the cuFile state. \n\n\n\n\n Note:\n\n           Not using the cuFileDeregister and cuFileDriverClose APIs (steps 5, 6, and 7) might unnecessarily consume resources, as shown by tools such as valgrind. The best practice is to always call these APIs in the application cleanup paths. \n         \n\n\n\n \n\n\n\n2.3.\u00a0cuFile Compatibility Mode\n\n\nUse Cases \n        \n         cuFile APIs can be used in different scenarios: \n         \nDevelopers building GPUDirect Storage applications with cuFile APIs, but don\u2019t have the supported hardware configurations. \nDevelopers building applications running on GPU cards that have CUDA compute capability > 6, but don\u2019t have BAR space exposed. \nDeployments where nvidia-fs.ko is not loaded or cannot be loaded. \nDeployments where the Linux distribution does not support GPUDirect Storage. \nDeployments where the filesystem may be not supported with GPUDirect Storage. \nDeployments where the network links are not enabled with RDMA support.\nDeployment where the configuration is not optimal for GPUDirect Storage. \n\nBehavior \n        \n         The cuFile library provides a mechanism for cuFile reads and writes to use compatibility mode using POSIX pread, pwrite, and aio_submit APIS respectively to host memory and copying to GPU memory when applicable. The behavior of compatibility mode with cuFile APIs is determined by the following configuration parameters. \n         \n\n\n\n\nConfiguration Option (default)\n cuFile IO Behavior\n\n\n\n\n\"allow_compat_mode\": true\nIf true, falls back to using compatibility mode when the library detects that the buffer file descriptor opened cannot use GPUDirect Storage. \n\n\n\"force_compat_mode\": false\nIf true, this option can be used to force all IO to use compatibility mode. Alternatively the admin can unload the nvidia_fs.ko or not expose the character devices in the docker container environment. \n\n\n\"gds_rdma_write_support\": true\nIf false, forces compatibility mode to be used for writes even when the underlying file system is capable of performing GPUDirect Storage writes. Note: If the option is \u201cfalse\u201d, this option will override and disable any filesystem-specific option to enable RDMA writes.  \n\n\n\"posix_unaligned_writes\" : false\n If true, forces compatibility mode to be used for writes where the file offset and/or IO size is not aligned to Page Boundary (4KB).  \n\n\n\u201clustre:posix_gds_min_kb\" : 0\n For a lustre filesystem, if greater than 0, compatibility mode is used for IO sizes between [1 - posix_gds_min_kb] specified in kB. Note: This option will force posix mode even if \u201callow_compat_mode\u201d is set to \u201cfalse\u201d. \n\n\n\"weka:rdma_write_support\" : false\n If this option is false, all writes to WekaFS will use compatibility mode. Note: If the option is set to \u201cfalse\u201d, cuFile library will use the posix path even if the allow_compat_mode option is true or false. \n\n\n\"gpfs:gds_write_support\" : false\n  If this option is false, all writes to IBM Spectrum Scale will use compatibility mode. Note: If the option is set to \u201cfalse\u201d, cuFile library will use the posix path even if the allow_compat_mode option is true or false. \n\n\n \n\"rdma_dynamic_routing\": false, \n\"rdma_dynamic_routing_order\": [ \" \"SYS_MEM\" ] \nIf rdma_dynamic_routing is set to true and rdma_dynamic_routing_order is set to [\u201cSYS_MEM\u201d], then all IO for DFS will use compatibility mode. \n\n\n\n\n\n\nIn addition to the above configuration options, compatibility mode will be used as a fallback option for following use cases.  \n         \n         \n\n\n\n\nUse Case\ncuFile IO Behavior\n\n\n\n\nNo BAR1 memory in GPU.\nUse compatibility mode.\n\n\n For wekaFS or IBM Spectrum Scale mounts: If there are no rdma_dev_addr_list specified, or failure to register MR with ib device.  \nUse compatibility mode.\n\n\nBounce buffers cannot be allocated in GPU memory.\nUse compatibility mode.\n\n\nFor WekaFS and IBM Spectrum Scale: If the kernel returns -ENOTSUP for GPUDirect Storage read/write. \nRetry the IO operation internally using compatibility mode. \n\n\ncuFile Stream and cuFile Batch APIs on IBM Spectrum Scale or WekaFS \nAll Async and batch operations will internally use compatibility mode IO. \n\n\nThe nvidia_fs.ko driver is not loaded. \nAll IO operations will use compatibility mode.\n\n\n\n\n\n\nLimitations\n\nCompatible mode does not work in cases where the GPUs have CUDA compute capability less than 6. \nGDS Compat mode has been tested and works with GDS enabled file systems and environments. It has not been tested to work on all other filesystems. \n\n\n\n\n\n\n3. cuFile API Specification\n\n\n \n       \n       \n       This section provides information about the cuFile APIs that are used from the CPU to enable applications and frameworks. \n \n\n\n\n3.1.\u00a0Data Types\n\n\n\n \n\n\n\n3.1.1.\u00a0Declarations and Definitions\n\n\n \n         \n         Here are the relevant cuFile enums and their descriptions.\n\n\n\nCopy\nCopied!\n\n\n\n\n\n\n\n\n            \n            typedef struct CUfileError {\n        CUfileOpError err; // cufile error\n        enum CUresult cu_err; // for CUDA-specific errors\n} CUfileError_t;\n\n/**\n * error macros to inspect error status of type CUfileOpError\n */\n \n#define IS_CUFILE_ERR(err) \\\n        (abs((err)) > CUFILEOP_BASE_ERR)\n \n#define CUFILE_ERRSTR(err) \\\n        cufileop_status_error(static_cast<CUfileOpError>(abs((err))))\n \n#define IS_CUDA_ERR(status) \\\n        ((status).err == CU_FILE_CUDA_DRIVER_ERROR)\n \n#define CU_FILE_CUDA_ERR(status) ((status).cu_\n        \n\n\n\n\n\nThe following enum and two structures enable broader cross-OS support:\n         \n\n\n\nCopy\nCopied!\n\n\n\n\n\n\n\n\n            \n            enum CUfileFileHandleType { \n    CU_FILE_HANDLE_TYPE_OPAQUE_FD = 1, /* linux based fd    */\n    CU_FILE_HANDLE_TYPE_OPAQUE_WIN32 = 2, /* windows based handle */\nCU_FILE_HANDLE_TYPE_USERSPACE_FS  = 3, /* userspace based FS */\n}; \n \ntypedef struct CUfileDescr_t {\nCUfileFileHandleType type; /* type of file being registered */\nunion { \nint fd;             /* Linux   */\nvoid *handle;         /* Windows */\n} handle;\nconst CUfileFSOps_t *fs_ops;     /* file system operation table */ \n}CUfileDescr_t;\n \n/* cuFile handle type */\ntypedef void*  CUfileHandle_t;\n \ntypedef struct cufileRDMAInfo\n{\n        int version;\n        int desc_len;\n        const char *desc_str;\n}cufileRDMAInfo_t;\n \ntypedef struct CUfileFSOps {\n      /* NULL means discover using fstat */\n      const char* (*fs_type) (void *handle);\n \n      /* list of host addresses to use,  NULL means no restriction */\n      int (*getRDMADeviceList)(void *handle, sockaddr_t **hostaddrs);\n \n      /* -1 no pref */\n      int (*getRDMADevicePriority)(void *handle, char*, size_t,\n                                loff_t, sockaddr_t* hostaddr);\n \n      /* NULL means try VFS */\n      ssize_t (*read) (void *handle, char*, size_t, loff_t, cufileRDMAInfo_t*);\n      ssize_t (*write) (void *handle, const char *, size_t, loff_t , cufileRDMAInfo_t*);\n}CUfileFSOps_t;\n\ntypedef enum CUfileDriverStatusFlags {\n        CU_FILE_LUSTRE_SUPPORTED = 0,        /*!< Support for DDN LUSTRE */\n        CU_FILE_WEKAFS_SUPPORTED = 1,        /*!< Support for WEKAFS */\n        CU_FILE_NFS_SUPPORTED = 2,           /*!< Support for NFS */\n        CU_FILE_GPFS_SUPPORTED = 3,          /*! < Support for GPFS */\n        CU_FILE_NVME_SUPPORTED = 4,          /*!< Support for NVMe */\n        CU_FILE_NVMEOF_SUPPORTED = 5,        /*!< Support for NVMeOF */\n        CU_FILE_SCSI_SUPPORTED = 6,          /*!< Support for SCSI */\n        CU_FILE_SCALEFLUX_CSD_SUPPORTED = 7, /*!< Support for Scaleflux CSD*/\n        CU_FILE_NVMESH_SUPPORTED = 8,        /*!< Support for NVMesh Block Dev*/\n        CU_FILE_BEEGFS_SUPPORTED = 9,        /*!< Support for BeeGFS */\n}CUfileDriverStatusFlags_t;\n\n \nenum CUfileDriverControlFlags {\n      CU_FILE_USE_POLL_MODE = 0, /*!< use POLL mode. properties.use_poll_mode*/\n      CU_FILE_ALLOW_COMPAT_MODE = 1 /*!< allow COMPATIBILITY mode. properties.allow_compat_mode*/\n};\n \ntypedef enum CUfileFeatureFlags {\n    CU_FILE_DYN_ROUTING_SUPPORTED =0,\n    CU_FILE_BATCH_IO_SUPPORTED = 1,\n    CU_FILE_STREAMS_SUPPORTED = 2\n} CUfileFeatureFlags_t;;\n \n/* cuFileDriverGetProperties describes this structure\u2019s members */\ntypedef struct CUfileDrvProps {\n   struct {\n     unsigned int major_version;\n     unsigned int minor_version;\n     size_t poll_thresh_size;\n     size_t max_direct_io_size;\n     unsigned int dstatusflags;\n     unsigned int dcontrolflags;\n   } nvfs;\n   CUfileFeatureFlags_t fflags;\n   unsigned int max_device_cache_size;\n   unsigned int per_buffer_cache_size;\n   unsigned int max_pinned_memory_size;\n   unsigned int max_batch_io_timeout_msecs;\n}CUfileDrvProps_t;\n\n/* Parameter block for async cuFile IO */ \n/* Batch APIs use an array of these    */\n/* Status must be CU_FILE_WAITING when submitted, and is\n   updated when enqueued and when complete, so this user-allocated\n   structure is live until the operation completes.    */\ntypedef enum CUFILEStatus_enum {\n        CUFILE_WAITING = 0x000001,  /* required value prior to submission */\n        CUFILE_PENDING = 0x000002,  /* once enqueued */\n        CUFILE_INVALID = 0x000004,  /* request was ill-formed or could not be enqueued */\n        CUFILE_CANCELED = 0x000008, /* request successfully canceled */\n        CUFILE_COMPLETE = 0x0000010, /* request successfully completed */\n        CUFILE_TIMEOUT = 0x0000020,  /* request timed out */\n        CUFILE_FAILED  = 0x0000040  /* unable to complete */\n}CUfileStatus_t;\n\ntypedef enum cufileBatchMode {\n        CUFILE_BATCH = 1,\n} CUfileBatchMode_t;\n \ntypedef struct CUfileIOParams {\n        CUfileBatchMode_t mode; // Must be the very first field.\n        union {\n                struct  {\n                        void *devPtr_base;\n                        off_t file_offset;\n                        off_t devPtr_offset;\n                        size_t size;\n                }batch;\n        }u;\n        CUfileHandle_t fh;\n        CUfileOpcode_t opcode;\n        void *cookie;\n}CUfileIOParams_t;\n \ntypedef struct CUfileIOEvents {\n        void *cookie;\n        CUfileStatus_t   status;      /* status of the operation */\n        size_t ret;       /* -ve error or amount of I/O done. */\n}CUfileIOEvents_t;\n        \n\n\n\n \n\n\n\n3.1.2.\u00a0Typedefs\n\n\n \n         \n         cuFile typedefs:\n\n\n\n\n\nCopy\nCopied!\n\n\n\n\n\n\n\n\n            \n            typedef struct CUfileDescr CUfileDesr_t\ntypedef struct CUfileError CUfileError_t\ntypedef struct CUfileDrvProps CUfileDrvProps_t\ntypedef enum CUfileFeatureFlags CUfileFeatureFlags_t\ntypedef enum CUfileDriverStatusFlags_enum CUfileDriverStatusFlags_t\ntypedef enum CUfileDriverControlFlags_enum CUfileDriverControlFlags_t\ntypedef struct CUfileIOParams CUfileIOParams_t\ntypedef enum CUfileBatchOpcode CUfileBatchOpcode_t\n        \n\n\n\n \n\n\n\n3.1.3.\u00a0Enumerations\n\n\n \n         \n         cuFile enums:\n\n\nenum CUfileOpcode_enumThis is the cuFile operation code for batch mode.\n\n\n\n\nOpCode\nValue\nDescription\n\n\n\n\nCU_FILE_READ\n0\nBatch Read\n\n\nCU_FILE_WRITE\n1\nBatch Write\n\n\n\n\n  \n\n\n\nCopy\nCopied!\n\n\n\n\n\n\n\n\n            \n            /* cuFile Batch IO operation kind */\nenum CUfileOpcode { \n     CU_FILE_READ,\n     CU_FILE_WRITE,\n};\n        \n\n\n\n\nenum CUfileStatusThe cuFile Status codes for batch mode. \n\n\n\n\nStatus\nValue\nDescription\n\n\n\n\nCUFILE_WAITING\n0x01\nThe initial value.\n\n\nCUFILE_PENDING\n0x02\nSet once enqueued into the driver.\n\n\nCUFILE_INVALID\n0x04\nInvalid parameters.\n\n\nCUFILE_CANCELED\n0x08\nRequest successfully canceled.\n\n\nCUFILE_COMPLETE\n0x10\nSuccessfully completed.\n\n\nCUFILE_TIMEOUT\n0x20\nThe operation has timed out.\n\n\nCUFILE_FAILED\n0x40\nIO has failed.\n\n\n\n\n   \n\nenum CUfileOpError\n\nThe cuFile Operation error types. \nAll error code values, other than CU_FILE_SUCCESS, are considered failures that might leave the output and input parameter values of APIs in an undefined state. These values cannot have any side effects on the file system, the application process, and the larger system. \n\n Note:\n\n               cuFile-specific errors will be greater than CUFILEOP_BASE_ERR to enable users to distinguish between POSIX errors and cuFile errors.\n              \n\n\n\nCopy\nCopied!\n\n\n\n\n\n\n\n\n            \n            #define CUFILEOP_BASE_ERR 5000\n        \n\n\n\n\n\n  \n\n\n\n\n\nError Code\nValue\nDescription\n\n\n\n\nCU_FILE_SUCCESS\n0\nThe cufile is successful.\n\n\nCU_FILE_DRIVER_NOT_INITIALIZED\n5001\nThe nvidia-fs driver is not loaded.\n\n\nCU_FILE_DRIVER_INVALID_PROPS\n5002\nAn invalid property.\n\n\nCU_FILE_DRIVER_UNSUPPORTED_LIMIT\n5003\nA property range error.\n\n\nCU_FILE_DRIVER_VERSION_MISMATCH\n5004\nAn nvidia-fs driver version mismatch.\n\n\nCU_FILE_DRIVER_VERSION_READ_ERROR\n5005\nAn nvidia-fs driver version read error.\n\n\nCU_FILE_DRIVER_CLOSING\n5006\nDriver shutdown in progress.\n\n\nCU_FILE_PLATFORM_NOT_SUPPORTED\n5007\nGDS is not supported on the current platform.\n\n\nCU_FILE_IO_NOT_SUPPORTED\n5008\nGDS is not supported on the current file.\n\n\nCU_FILE_DEVICE_NOT_SUPPORTED\n5009\nGDS is not supported on the current GPU.\n\n\nCU_FILE_NVFS_DRIVER_ERROR\n5010\nAn nvidia-fs driver ioctl error.\n\n\nCU_FILE_CUDA_DRIVER_ERROR\n5011\n  A CUDA Driver API error.  This error indicates a CUDA driver-api error. If this is set, a CUDA-specific error code is set in the cu_err field for cuFileError.  \n\n\nCU_FILE_CUDA_POINTER_INVALID\n5012\nAn invalid device pointer.\n\n\nCU_FILE_CUDA_MEMORY_TYPE_INVALID\n5013\nAn invalid pointer memory type.\n\n\nCU_FILE_CUDA_POINTER_RANGE_ERROR\n5014\nThe pointer range exceeds the allocated address range. \n\n\nCU_FILE_CUDA_CONTEXT_MISMATCH\n5015\nA CUDA context mismatch.\n\n\nCU_FILE_INVALID_MAPPING_SIZE\n5016\nAccess beyond the maximum pinned memory size.\n\n\nCU_FILE_INVALID_MAPPING_RANGE\n5017\nAccess beyond the mapped size.\n\n\nCU_FILE_INVALID_FILE_TYPE\n5018\nAn unsupported file type.\n\n\nCU_FILE_INVALID_FILE_OPEN_FLAG\n5019\nUnsupported file open flags.\n\n\nCU_FILE_DIO_NOT_SET\n5020\nThe fd direct IO is not set.\n\n\nCU_FILE_INVALID_VALUE\n5022\nInvalid API arguments.\n\n\nCU_FILE_MEMORY_ALREADY_REGISTERED\n5023\nDevice pointer is already registered.\n\n\nCU_FILE_MEMORY_NOT_REGISTERED\n5024\nA device pointer lookup failure has occurred.\n\n\nCU_FILE_PERMISSION_DENIED\n5025\nA driver or file access error. \n\n\nCU_FILE_DRIVER_ALREADY_OPEN\n5026\nThe driver is already open.\n\n\nCU_FILE_HANDLE_NOT_REGISTERED\n5027\nThe file descriptor is not registered.\n\n\nCU_FILE_HANDLE_ALREADY_REGISTERED\n5028\nThe file descriptor is already registered.\n\n\nCU_FILE_DEVICE_NOT_FOUND\n5029\nThe GPU device cannot be not found.\n\n\nCU_FILE_INTERNAL_ERROR\n5030\nAn internal error has occurred. Refer to cufile.log for more details. \n\n\nCU_FILE_GETNEWFD_FAILED\n5031\nFailed to obtain a new file descriptor.\n\n\nCU_FILE_NVFS_SETUP_ERROR\n5033\nAn NVFS driver initialization error has occurred.\n\n\nCU_FILE_IO_DISABLED\n5034\nGDS is disabled by config on the current file.\n\n\nCU_FILE_BATCH_SUBMIT_FAILED\n5035\nFailed to submit a batch operation.\n\n\nCU_FILE_GPU_MEMORY_PINNING_FAILED\n5036\nFailed to allocate pinned GPU memory.\n\n\nCU_FILE_BATCH_FULL\n5037\nQueue full for batch operation.\n\n\nCU_FILE_ASYNC_NOT_SUPPORTED\n5038\ncuFile stream operation is not supported.\n\n\n\n\n \n\n Note:\n\n             Data path errors are captured via standard error codes by using errno. The APIs will return -1 on error. \n           \n\n  \n\n\n \n\n\n\n3.2.\u00a0cuFile Driver APIs\n\n\n \n        \n        The following cuFile APIs that are used to initialize, finalize, query, and tune settings for the cuFile system. \n\n\n\n\n\nCopy\nCopied!\n\n\n\n\n\n\n\n\n            \n            /* Initialize the cuFile infrastructure */\nCUfileError_t cuFileDriverOpen();  \n\n/* Finalize the cuFile system */\nCUfileError_t cuFileDriverClose();\n\n/* Query capabilities based on current versions, installed functionality */\nCUfileError_t cuFileGetDriverProperties(CUfileDrvProps_t *props);\n\n/*API to set whether the Read/Write APIs use polling to do IO operations */\nCUfileError_t cuFileDriverSetPollMode(bool poll, size_t poll_threshold_size);\n\n/*API to set max IO size(KB) used by the library to talk to nvidia-fs driver */\nCUfileError_t cuFileDriverSetMaxDirectIOSize(size_t max_direct_io_size);\n\n/* API to set maximum GPU memory reserved per device by the library for internal buffering */\nCUfileError_t cuFileDriverSetMaxCacheSize(size_t max_cache_size);\n\n/* Sets maximum buffer space that is pinned in KB for use by  cuFileBufRegister\nCUfileError_t cuFileDriverSetMaxPinnedMemSize(size_t\n   max_pinned_memory_size);\n        \n\n\n\n\n\n\n Note:\n\n          Refer to sample_007 for usage. \n        \n\n\n \n\n\n\n3.3.\u00a0cuFile Synchronous IO APIs\n\n\n \n        \n        The core of the cuFile IO APIs are the read and write functions.\n\n\n\n\n\nCopy\nCopied!\n\n\n\n\n\n\n\n\n            \n            ssize_t cuFileRead(CUFileHandle_t fh, void *bufPtr_base, size_t size, off_t file_offset, off_t devPtr_offset);\nssize_t cuFileWrite(CUFileHandle_t fh, const void *bufPtr_base, size_t size, off_t file_offset, off_t devPtr_offset);\n        \n\n\n\n \n        \n         The starting offset of the buffer on the device or host is determined by a base (bufPtr_base) and offset (bufPtr_offset). This offset is distinct from the offset in the file. \n         \n\n Note:\n\n           To use the registered buffer, the bufPtr_base must be the buffer pointer used to register during cuFileBufRegister. Otherwise cuFileRead and cuFileWrite APIs may use internal memory buffers for GPUDirect Storage peer to peer operations. \n         \n\n\n\n\n\n Note:\n\n          The default behavior for all paths where GDS is not supported is for the cuFile IO API to attempt IO using file system supported posix mode APIs when properties.allow_compat_mode is set to true. In order to disable cuFile APIs falling back to posix APIs for unsupported GDS paths, properties.allow_compat_mode in the /etc/cufile.json file should be set to false. \n        \n\n\n\n\n Note:\n\n          Refer to sample sample_003 for usage. \n        \n\n\n \n\n\n\n3.4.\u00a0cuFile File Handle APIs\n\n\n \n        \n        Here is some information about the cuFile Handle APIs.\nThe cuFileHandleRegister API makes a file descriptor or handle that is known to the cuFile subsystem by using an OS-agnostic interface. The API returns an opaque handle that is owned by the cuFile subsystem.  \n        \nTo conserve memory, the cuFileHandleDeregister API is used to release cuFile-related memory objects. Using only the POSIX close will not clean up resources that were used by cuFile. Additionally, the clean up of cuFile objects associated with the files that were operated on in the cuFile context will occur at cuFileDriverClose.  \n        \n         \n\n\n\nCopy\nCopied!\n\n\n\n\n\n\n\n\n            \n            CUfileError_t cuFileHandleRegister(CUFileHandle_t *fh, CUFileDescr_t *descr);\nvoid cuFileHandleDeregister(CUFileHandle_t fh);\n        \n\n\n\n\n\n\n Note:\n\n          Refer to sample_003 for usage. \n        \n\n\n \n\n\n\n3.5.\u00a0cuFile Buffer APIs\n\n\n \n        \n        The cuFileBufRegister API incurs a significant performance cost, so registration costs should be amortized where possible. Developers must ensure that buffers are registered up front and off the critical path. \nThe cuFileBufRegister API is optional. If this is not used, instead of pinning the user\u2019s memory, cuFile-managed and internally pinned buffers are used.  \n        \nThe cuFileBufDeregister API is used to optimally clean up cuFile-related memory objects, but CUDA currently has no analog to cuFileBufDeregister. The cleaning up of objects associated with the buffers operated on in the cuFile context occurs at cuFileDriverClose. If explicit APIs are used, the incurred errors are reported immediately, but if the operations of these explicit APIs are performed implicitly, error reporting and handling are less clear. \n        \n\n\n\nCopy\nCopied!\n\n\n\n\n\n\n\n\n            \n            CUfileError_t cuFileBufRegister(const void *devPtr_base, size_t size, int flags);\nCUfileError_t cuFileBufDeregister(const void *devPtr_base);\n        \n\n\n\n\n\n Note:\n\n          Refer to sample_005 for usage. \n        \n\n\n \n\n\n\n3.6.\u00a0cuFile Stream APIs\n\n\n \n        \n        Operations that are enqueued with cuFile Stream APIs are FIFO ordered with respect to other work on the stream and must be completed before continuing with the next action in the stream. \n\n\n\n\n\nCopy\nCopied!\n\n\n\n\n\n\n\n\n            \n            CUfileError_t cuFileReadAsync(CUFileHandle_t fh, void *bufPtr_base, \n                  size_t *size_p, off_t *file_offset_p, off_t *bufPtr_offset_p,\n                  ssize_t *bytes_read_p, CUStream stream);\nCUfileError_t cuFileWriteAsync(CUFileHandle_t fh, void *bufPtr_base, \n                  size_t *size_p, off_t *file_offset_p, off_t *bufPtr_offse_pt,\n                  ssize_t *bytes_written_p, CUstream stream);\n        \n\n\n\n\n\n Note:\n\n          Refer to samples sample_031, sample_032, sample_033, and sample_034 for usage. \n        \n\n\n \n\n\n\n3.7.\u00a0cuFile Batch APIs\n\n\n \n        \n        Batch APIs are submitted synchronously, but executed asynchronously with respect to host thread.\n \n        \n          These operations can be submitted on different files, different locations in the same file, or a mix. Completion of IO can be checked asynchronously using a status API in the same host thread or in a different thread. The cuFileBatchIOGetStatus API takes an array of CUfileIOEvents_t and minimum number of elements to poll for. which describes the IO action, status, errors, and bytes transacted for each instance. The bytes transacted field is valid only when the status indicates a successful completion. \n         \n\n Note:\n\n           Refer to samples sample_019, sample_020, sample_021, and sample_022 for usage. \n         \n\n\n\n\n\n\n\n4. cuFile API Functional Specification\n\n\n \n       \n       \n       This section provides information about the cuFile API functional specification. \nSee the GPUDirect Storage Overview Guide for a high-level analysis of the set of functions and their relation to each other. We anticipate adding additional return codes for some of these functions.  \n       All cuFile APIs are called from the host code. \n      \n \n\n\n4.1.\u00a0cuFileDriver API Functional Specification\n\n\n \n       This section provides information about the cuFileDriver API functional specification.  \n\n\n\n4.1.1.\u00a0cuFileDriverOpen\n\n\n\n\n\n\nCopy\nCopied!\n\n\n\n\n\n\n\n\n            \n            CUfileError_t cuFileDriverOpen();\n        \n\n\n\n\nOpens the Driver session to support GDS IO operations. \n         Parameters\n\nNone\n\nReturns\n\n\nCU_FILE_SUCCESS on a successful open, or if the driver is already open. \n\nCU_FILE_DRIVER_NOT_INITIALIZED on a failure to open the driver. \n\nCU_FILE_PERMISSION_DENIED on a failure to open. This can happen when the character device (/dev/nvidia_fs[0-15]) is restricted to certain users by an administrator, for example, admin, where /dev is not exposed with read permissions in the container.  \n  CU_FILE_DRIVER_VERSION_MISMATCH, when there is a mismatch between the cuFile library and its kernel driver.  \n\nCU_FILE_CUDA_DRIVER_ERROR if the CUDA driver failed to initialize. CU_FILE_PLATFORM_NOT_SUPPORTED if the current platform is not supported by GDS. \n\nCU_FILE_NVFS_SETUP_ERROR for a cuFile-specific internal error. \n\nRefer to the cufile.log file for more information.  \n         Description\n\nThis API opens the session with the NVFS kernel driver to communicate from userspace to kernel space and calls the GDS driver to set up the resources required to support GDS IO operations. \nThe API checks whether the current platform supports GDS and initializes the cuFile library. \nThis API loads the cuFile settings from a JSON configuration file in /etc/cufile.JSON. If the JSON configuration file does not exist, the API loads the default library settings. To modify this default config file, administrative privileges are needed. The administrator can modify it to grant cuFile access to the specified devices and mount paths and also tune IO parameters (in KB, 4K aligned) that are based on the type of workload. Refer to the default config file (/etc/cufile.json) for more information.  \n\n \n\n\n\n4.1.2.\u00a0cuFileDriverClose\n\n\n\n\n\n\nCopy\nCopied!\n\n\n\n\n\n\n\n\n            \n            CUfileError_t cuFileDriverClose();\n        \n\n\n\n\n\nCloses the driver session and frees any associated resources for GDS.\nThis happens implicitly upon process exit.\nThe driver can be reopened once it is closed.\n\nParameters\n\nNone\n\nReturns\n\n\nCU_FILE_SUCCESS on a successful close. \n\nCU_FILE_DRIVER_NOT_INITIALIZED on failure. \n\nDescription\n\nClose the GDS session and any associated memory resources. If there are buffers registered by using cuFileBufRegister, which are not unregistered, a cuFileDriverClose implicitly unregisters those buffers. Any in-flight IO when cuFileDriverClose is in-progress will receive an error. \n\n \n\n\n\n4.1.3.\u00a0cuFileDriverGetProperties\n\n\n \n         \n         The cuFileDrvProps_t structure can be queried with cuFileDriverGetProperties and selectively modified with cuFileDriverSetProperties. The structure is self-describing, and its fields are consistent with the major and minor API version parameters. \n\n\n\nCopy\nCopied!\n\n\n\n\n\n\n\n\n            \n            CUfileError_t cuFileDriverGetProperties(cuFileDrvProps_t *props);\n        \n\n\n\n\n\nGets the Driver session properties for GDS functionality.\n\nParameters\nprops\n\nPointer to the cuFile Driver properties.\n\nReturns\n\n\nCU_FILE_SUCCESS on a successful completion. \n\nCU_FILE_DRIVER_NOT_INITIALIZED on failure. \n\nCU_FILE_DRIVER_VERSION_MISMATCH on a driver version mismatch. \n\nCU_FILE_INVALID_VALUE if input is invalid. \n\nDescription\nThis API is used to get current GDS properties and nvidia-fs driver properties and functionality, such as support for SCSI, NVMe, and NVMe-OF.  \n         This API is used to get the current nvidia-fs drivers-specific properties such as the following:  \n         \n\nmajor_version: the cuFile major version \n\nminor_version: the cuFile minor version \n\nprops.nvfs.dstatusflags, which are bit flags that indicate support for the following driver features: \n  \n CU_FILE_EXASCALER_SUPPORTED, a bit to check whether the DDN EXAScaler parallel filesystem solutions (based on the Lustre filesystem) client supports GDS. \n CU_FILE_WEKAFS_SUPPORTED, a bit to check whether WekaFS supports GDS. \n \n\nProps.nvfs.dcontrolflags, which are bit flags that indicate the current activation for driver features: \n  \n CU_FILE_USE_POLL_MODE, when bit is set, IO uses polling mode. \n CU_FILE_ALLOW_COMPAT_MODE, if the value is 1 compatible mode is set. Otherwise, the compatible mode is disabled. \n \n\nProps.fflags, which are bit flags that indicate whether the following library features are supported: \n  \n CU_FILE_STREAMS_SUPPORTED, an attribute that checks whether CUDA-streams are supported. \n CU_FILE_DYN_ROUTING_SUPPORTED, an attribute that checks whether dynamic routing feature is supported. \n \n\nProps.nvfs.poll_thresh_size, a maximum IO size, in KB and must be 4K-aligned, that is used for the POLLING mode. \n\nProps.nvfs.max_direct_io_size, a maximum GDS IO size, in KB and must be 4K-aligned, that is requested by the nvidia-fs driver to the underlying filesystem. \n\nProps.max_device_cache_size, a maximum GPU buffer space per device, in KB and must be 4K-aligned. Used internally, for example, to handle unaligned IO and optimal IO path routing. This value might be rounded down to the nearest GPU page size. \n\nProps.max_device_pinned_mem_size, a maximum buffer space, in KB and must be 4K-aligned, that is pinned and mapped to the GPU BAR space. This might be rounded down to the nearest GPU page size. \n\nProps.per_buffer_cache_size, a GPU bounce buffer size, in KB, used for internal pools. \n\nAdditional Information \n         \n          See the following for more information: \n          \ncuFileDriverSetPollMode(bool poll, size_t poll_threshold_size)\ncuFileDriverSetMaxDirectIOSize(size_t max_direct_io_size)\ncuFileDriverSetMaxCacheSize(size_t max_cache_size)\ncuFileDriverSetMaxPinnedMemSize(size_t max_pinned_memory_size)\n\n\n \n\n\n\n4.1.4.\u00a0cuFileDriverSetPollMode(bool poll, size_t poll_threshold_size)\n\n\n\ncuFileDriverSetPollMode(bool poll, size_t poll_threshold_size) API \n\n\n\nCopy\nCopied!\n\n\n\n\n\n\n\n\n            \n            CUfileError_t cuFileDriverSetPollMode(bool poll,\n                                       size_t poll_threshold_size);\n        \n\n\n\n\n\nSets whether the Read/Write APIs use polling to complete IO operations. If poll mode is enabled, an IO size less than or equal to the threshold value is used for polling. \nThe poll_threshold_size must be 4K aligned. \n\nParameters\npoll\n\nBoolean to indicate whether to use the poll mode.\n\npoll_threshold_size\n\nIO size to use for POLLING mode in KB.\nThe default value is 4KB.\n\nReturns\n\n\nCU_FILE_SUCCESS on a successful completion. \n\nCU_FILE_DRIVER_NOT_INITIALIZED on failure to load the driver. \n\nCU_FILE_DRIVER_UNSUPPORTED_LIMIT on failure to set with valid threshold size \n\nDescription\nThis API is used in conjunction with cuFileGetDriverProperties. This API is used to set whether the library should use polling and the maximum IO threshold size less than or equal to which it will poll.  \n         This API overrides the default value that may be set through the JSON configuration file using the config keys properties.poll_modeand properties.poll_max_size_kb for the current process.  \n         \n          See the following for more information: \n          \ncuFileDriverGetProperties\n\n\n \n\n\n\n4.1.5.\u00a0cuFileDriverSetMaxDirectIOSize(size_t max_direct_io_size)\n\n\n\n\n\n\nCopy\nCopied!\n\n\n\n\n\n\n\n\n            \n            CUfileError_t cuFileDriverSetMaxDirectIOSize(size_t max_direct_io_size);\n        \n\n\n\n\n\nSets the max IO size, in KB. This parameter is used by the nvidia-fs driver as the maximum IO chunk size in which IO is issued to the underlying filesystem. In compatible mode, this is the maximum IO chunk size that the library uses to issue POSIX read/writes.  \nThe max direct IO size must be 4K aligned.\n\nParameters\nmax_direct_io_size\n\nThe maximum allowed direct IO size in KB.\nThe default value is 16384KB. This is because typically parallel-file systems perform better with bulk read/writes. \n\nReturns\n\n\nCU_FILE_SUCCESS on successful completion. \n\nCU_FILE_DRIVER_NOT_INITIALIZED on failure to load the driver. \n\nCU_FILE_DRIVER_UNSUPPORTED_LIMIT on failure to set with valid size. \n\nDescription\nThis API is used with cuFileGetDriverProperties and is used to set the maximum direct IO size used by the library to specify the nvidia-fs kernel driver the maximum chunk size in which the latter can issue IO to the underlying filesystem. In compatible mode, this is the maximum IO chunk size which the library uses for issuing POSIX read/writes. This parameter is dependent on the underlying GPU hardware and system memory.  \n         This API overrides the default value that might be set through the JSON configuration file by using the properties.max_direct_io_size_kb config key for the current process.  \n         \n          Refer to the following for more information: \n          \ncuFileDriverGetProperties\n\n\n \n\n\n\n4.1.6.\u00a0cuFileDriverSetMaxCacheSize(size_t max_cache_size)\n\n\n\n\n\n\nCopy\nCopied!\n\n\n\n\n\n\n\n\n            \n            CUfileError_t cuFileDriverSetMaxCacheSize(size_t max_cache_size);\n        \n\n\n\n\n\nSets the maximum GPU buffer space, in KB, per device and is used for internal use, for example, to handle unaligned IO and optimal IO path routing. This value might be rounded down to the nearest GPU page size. \nThe max cache size must be 4K aligned.\nThis API overrides the default value that might be set through the JSON configuration file using the properties.max_device_cache_size_kb config key for the current process. \n\nParameters\nmax_cache_size\n\nThe maximum GPU buffer space, in KB, per device used for internal use, for example, to handle unaligned IO and optimal IO path routing. This value might be rounded down to the nearest GPU page size. \nThe default value is 131072KB.\n\nReturns\n\n\nCU_FILE_SUCCESS on successful completion. \n\nCU_FILE_DRIVER_NOT_INITIALIZED on failure to load the driver. \n\nCU_FILE_DRIVER_UNSUPPORTED_LIMIT on failure to set with valid IO size \n\nDescription\nThis API is used with cuFileGetDriverProperties and is used to set the upper limit on the cache size per device for internal use by the library.  \n         \nSee cuFileDriverGetProperties for more information.  \n         \n\n\n\n4.1.7.\u00a0cuFileDriverSetMaxPinnedMemSize(size_t max_pinned_memory_size)\n\n\n\n\n\n\nCopy\nCopied!\n\n\n\n\n\n\n\n\n            \n            CUfileError_t cuFileDriverSetMaxPinnedMemSize(size_t max_pinned_mem_size);\n        \n\n\n\n\n\nSets the maximum GPU buffer space, in KB, that is pinned and mapped. This value might be rounded down to the nearest GPU page size. \nThe max pinned size must be 4K aligned.\nThe default value corresponds to the maximum PinnedMemory or the physical memory size of the device. \nThis API overrides the default value that may be set by the properties.max_device_pinned_mem_size_kb JSON config key for the current process. \n\nParameters\nmax_pinned_memory_size\n\nThe maximum buffer space, in KB, that is pinned and mapped to the GPU BAR space. \nThis value might be rounded down to the nearest GPU page size.\nThe maximum limit may be set to UINT64_MAX, which is equivalent to no enforced limit. It may be set to something smaller than the size of the GPU\u2019s physical memory. \n\nReturns\n\n\nCU_FILE_SUCCESS on successful completion. \n\nCU_FILE_DRIVER_NOT_INITIALIZED on failure to load driver. \n\nCU_FILE_DRIVER_UNSUPPORTED_LIMIT on failure to set with valid size. \n\nDescription\n\nThis API is used with cuFileGetDriverProperties and is used to set an upper limit on the maximum size of GPU memory that can be pinned and mapped and is dependent on the underlying GPU hardware and system memory. This API is related to cuFileBufRegister, which is used to register GPU device memory. SeecuFileDriverGetProperties for more information.  \n        \n \n\n\n\n4.2.\u00a0cuFile IO API Functional Specification\n\n\n \n        \n        This section provides information about the cuFile IO API function specification. \nThe device pointer addresses referred to in these APIs pertain to the current context for the caller.  \n        Unlike the non-async version of cuMemcpy, the cuFileHandleRegister, cuFileHandleDeregister, cuFileRead, and cuFileWrite APIs do not have the semantic of being ordered with respect to other work in the null stream.  \n       \n \n\n\n4.2.1.\u00a0cuFileHandleRegister\n\n\n\n\n\n\nCopy\nCopied!\n\n\n\n\n\n\n\n\n            \n            CUfileError_t cuFileHandleRegister(CUFileHandle_t *fh, CUfileDescr_t *descr);\n        \n\n\n\n\n\nRegister an open file.\n\ncuFileHandleRegister is required and performs extra checking that is memoized to provide increased performance on later cuFile operations. \nThis API is OS agnostic. \n    \n Note:\n\n             CUDA toolkit 12.2 (GDS version 1.7.x) supports non O_DIRECT open flags as well as O_DIRECT. Application is allowed to open a file in non O_DIRECT mode in compat mode and also with nvidia-fs.ko installed. In the latter case, an O_DIRECT path between GPU and Storage will be used if such a path exists. \n           \n\n\nParameters\n\n\n\nfhValid pointer to the OS-neutral cuFile handle structure supplied by the user but populated and maintained by the cuFile runtime.  \n\ndescValid pointer to the OS-neutral file descriptor supplied by the user carrying details regarding the file to be opened such as fd for Linux-based files.  \n\nReturns\n\n\nCU_FILE_SUCCESS on successful completion. \n\nCU_FILE_DRIVER_NOT_INITIALIZED on failure to load the driver. \n\nCU_FILE_IO_NOT_SUPPORTED, if the filesystem is not supported. \n\nCU_FILE_INVALID_VALUE if there are null or bad API arguments. \n\nCU_FILE_INVALID_FILE_OPEN_FLAG, if the file is opened with unsupported modes such as no O_APPEND, O_NOCTTY, O_NONBLOCK, O_DIRECTORY, O_NOFOLLOW, O_NOATIME, and O_TMPFILE. \n\nCU_FILE_INVALID_FILE_TYPE, if the file path is not valid, not a regular file, not a symbolic link, or not a device file. \n\nCU_FILE_HANDLE_ALREADY_REGISTERED if the file is already registered using the same file-descriptor. \nDescription\n\n\nGiven a file-descriptor will populate and return the CUfileHandle_tneeded for issuing IO with cuFile APIs. \n A return value of anything other than CU_FILE_SUCCESS leaves fh in an undefined state but has no other side effects. \nBy default this API accepts whether the file descriptor is opened with O_DIRECT mode or non O_DIRECT mode. \n \n         \n          Refer to the following for more information: \n          \n\ncuFileRead\ncuFileWrite\ncuFileReadAsync\ncuFileWriteAsync\ncuFileHandleDeregister\n\n\n \n\n\n\ncuFileHandleDeregister\n\n\n\n\n\n\nCopy\nCopied!\n\n\n\n\n\n\n\n\n            \n            CUfileError_t cuFileHandleDeregister(CUFileHandle_t *fh);\n        \n\n\n\nParameters\n\n\n\nfhThe file handle obtained from cuFileHandleRegister.  \n\nReturns\nNone \n          \n          \n\n Note:\n\n            This API only logs an ERROR level message in the cufile.log file for valid inputs. \n          \n\nDescription\n\n\nThe API is used to release resources that are claimed by cuFileHandleRegister. This API should be invoked only after the application ensures there are no outstanding IO operations with the handle. If cuFileHandleDeregister is called while IO on the file is in progress might result in undefined behavior.  \nThe user is still expected to close the file descriptor outside the cuFile subsystem after calling this API using close system call. Closing a file handle without calling cuFileHandleDeregister does not release the resources that are held in the cuFile library. If this API is not called, the cuFile subsystem releases the resources lazily or when the application exits.  \n \n         \n          See the following for more information: \n          \ncuFileRead\ncuFileWrite\ncuFileHandleDeregister\n\n\n \n\n\n\n4.2.3.\u00a0cuFileRead\n\n\n\n\n\n\nCopy\nCopied!\n\n\n\n\n\n\n\n\n            \n            ssize_t cuFileRead(CUfileHandle_tfh, void *bufPtr_base, size_t size, off_t file_offset, off_t bufPtr_offset);\n        \n\n\n\n\n\nReads specified bytes from the file descriptor into the device memory or the host memory. \nParameters\n\n\n\nfhFile descriptor for the file. \n\nbufPtr_baseBase address of buffer in device memory or host memory. For registered buffers, bufPtr_base must remain set to the base address used in the cuFileBufRegister call.  \n\nsizeSize in bytes to read. \n\nfile_offsetOffset in the file to read from. \n\nbufPtr_offsetOffset relative to the bufPtr_base pointer to read into. This parameter should be used only with registered buffers.  \n\nReturns\n\nSize of bytes that were successfully read.\n-1 on an error, so errno is set to indicate filesystem errors. \nAll other errors return a negative integer value of the CUfileOpError enum value. \n\nDescription\nThis API reads the data from a specified file handle at a specified offset and size bytes into the GPU memory by using GDS functionality or into the host memory based on the type of memory pointer. The API works correctly for unaligned offsets and any data size, although the performance might not match the performance of aligned reads.This is a synchronous call and blocks until the IO is complete.  \n         \n\n Note:\n\n           For the bufPtr_offset, if data will be read starting exactly from the bufPtr_base that is registered with cuFileBufRegister, bufPtr_offset should be set to 0. To read starting from an offset in the registered buffer range, the relative offset should be specified in the bufPtr_offset, and the bufPtr_base must remain set to the base address that was used in the cuFileBufRegister call. \n         \n\n \n         \n          See the following for more information: \n          \ncuFileWrite\ncuFileReadAsync\ncuFileWriteAsync\n\n\n \n\n\n\n4.2.4.\u00a0cuFileWrite\n\n\n\n\n\n\nCopy\nCopied!\n\n\n\n\n\n\n\n\n            \n            ssize_t cuFileWrite(CUfileHandle_t fh, const void *bufPtr_base, size_t size, off_t file_offset, off_t bufPtr_offset);\n        \n\n\n\n\n\nWrites specified bytes from the device memory into the file descriptor using GDS. \nParameters\n\n\n\nfhFile descriptor for the file \n\nbufPtr_baseBase address of buffer in device memory or host memory. For registered buffers, bufPtr_base must remain set to the base address used in the cuFileBufRegister call.  \n\nsizeSize in bytes to which to write. \n\nfile_offsetOffset in the file to which to write. \n\nbufPtr_offsetOffset relative to the bufPtr_base pointer from which to write. This parameter should be used only with registered buffers.  \n\nReturns\n\nSize of bytes that were successfully written.\n-1 on an error, so errno is set to indicate filesystem errors. \nAll other errors return a negative integer value of the CUfileOpError enum value. \n\nDescription\nThis API writes the data from the GPU memory or the host memory to a file specified by the file handle at a specified offset and size bytes by using GDS functionality. The API works correctly for unaligned offset and data sizes, although the performance is not on-par with aligned writes.This is a synchronous call and will block until the IO is complete.  \n         \n\n Note:\n\n           GDS functionality modified the standard file system metadata in SysMem. However, GDS functionality does not take any special responsibility for writing that metadata back to permanent storage. The data is not guaranteed to be present after a system crash unless the application uses an explicit fsync(2) call. If the file is opened with an O_SYNC flag, the metadata will be written to the disk before the call is complete. \n         \n\n\nRefer to the note in cuFileRead for more information about bufPtr_offset:.  \n         \n          Refer to the following for more information: \n          \ncuFileWrite\ncuFileReadAsync\ncuFileWriteAsync\n\n\n\n \n\n\n\n4.3.\u00a0cuFile Memory Management Functional Specification\n\n\n \n        \n        The device pointer addresses that are mentioned in the APIs in this section pertain to the current context for the caller. cuFile relies on users to complete their own allocation before using the cuFileBufRegister API and free after using the cuFileBufDeregister API. \n \n\n\n\n4.3.1.\u00a0cuFileBufRegister\n\n\n\n\n\n\nCopy\nCopied!\n\n\n\n\n\n\n\n\n            \n            CUfileError_t cuFileBufRegister(const void *bufPtr_base,\n                                size_t size, int flags);\n        \n\n\n\n\n\nBased on the memory type, this API registers existing cuMemAlloc\u2019d (pinned) memory for GDS IO operations or host memory for IO operations. \nParameters\n\n\n\nbufPtr_baseAddress of device pointer. cuFileRead and cuFileWritemust use this bufPtr_base as the base address.  \n\nsizeSize in bytes from the start of memory to map. \n\nflagsReserved for future use, must be 0. \n\nReturns\n\n\nCU_FILE_SUCCESS on a successful registration. \n\nCU_FILE_NVFS_DRIVER_ERROR if the nvidia-fs driver cannot handle the request. \n\nCU_FILE_INVALID_VALUE on a failure. \n\nCU_FILE_CUDA_DRIVER_ERROR on CUDA-specific errors. CUresult code can be obtained using CU_FILE_CUDA_ERR(err). \n\nCU_FILE_MEMORY_ALREADY_REGISTERED, if memory is already registered. \n\nCU_FILE_INTERNAL_ERROR, an internal library-specific error. \n\nCU_FILE_CUDA_MEMORY_TYPE_INVALID, for device memory that is not allocated via cudaMalloc or cuMemAlloc. \n\nCU_FILE_CUDA_POINTER_RANGE_ERROR, if the size exceeds the bounds of the allocated memory. \n\nCU_FILE_INVALID_MAPPING_SIZE, if the size exceeds the GPU resource limits. \n\nCU_FILE_GPU_MEMORY_PINNING_FAILED, if not enough pinned memory is available. \n\nDescription\nBased on the memory type, this API either registers the specified GPU address or host memory address and size for use with the cuFileRead and cuFileWrite operations. The user must call cuFileBufDeregister to release the pinned memory mappings for GPU memory if needed.  \n         \n          See the following for more information: \n          \ncuFileBufDeregister\n\n\n \n\n\n\n4.3.2.\u00a0cuFileBufDeregister\n\n\n\n\n\n\nCopy\nCopied!\n\n\n\n\n\n\n\n\n            \n            CUfileError_t cuFileBufDeregister(const void *bufPtr_base);\n        \n\n\n\n\n\nBased on the memory type, this API either deregisters CUDA memory or the host memory registered using the cuFileBufRegister API. \nParameters\n\n\n\nbufPtr_baseAddress of device pointer to release the mappings that were provided to cuFileBufRegister \n\nReturns\n\n\nCU_FILE_SUCCESS on a successful deregistration. \n\nCU_FILE_MEMORY_NOT_REGISTERED, if bufPtr_base was not registered. \n\nCU_FILE_ERROR_INVALID_VALUE on failure to find the registration for the specified memory. \n\nCU_FILE_INTERNAL_ERROR, an internal library-specific error. \n\nDescription\n\nThis API deregisters memory mappings that were registered by cuFileBufRegister. Refer to cuFileBufRegister for more information.  \n        \n \n\n\n\n4.4.\u00a0cuFile Stream API Functional Specification\n\n\n \n        \n        This section provides information about the cuFile stream API functional specification. \nThe stream APIs are similar to Read and Write, but they take a stream parameter to support asynchronous operations and execute in the CUDA stream order.  \n       \n \n\n\n4.4.1.\u00a0cuFileStreamRegister\n\n\n\n\n\n\nCopy\nCopied!\n\n\n\n\n\n\n\n\n            \n            CUfileError_t cuFileStreamRegister(CUStream_t stream, unsigned flags);\n        \n\n\n\n\n\n Defines the input behavior for stream I/O APIs.  \nParameters\n\n\n\nstreamCUDA stream in which to enqueue the operation. If NULL, make this operation in the default CUDA stream.  \n\nflagsThe following are valid values:\n\n\n\n\nValue\nDescription\n\n\n\n\n0x0\nAll the I/O parameters are valid only at the time of execution. \n\n\n0x1\nBuffer offset value is valid at submission time. \n\n\n0x2\nFile offset value is valid at submission time. \n\n\n0x4\nSize is valid at submission time.\n\n\n0x8\nAll inputs i.e. buffer offset, file offset and size are 4K aligned. \n\n\n0xf\nAll inputs are aligned and known at submission time. \n\n\n\n\n   \n\n\n Note:\n\n           Using the flag \u20180XF\u2019 will perform best as the workflow can be optimized during submission time. \n         \n\n\n\nDescription\nThis optional API registers the stream with the cuFile subsystem.  \n         This API will allocate resources to handle stream operations for cuFile. \n         The API will synchronize on the stream before allocating resources. \n         The stream pointer is expected to be a valid pointer. \n          \n         Returns\n\n\nCU_FILE_SUCCESS on a successful submission. \n\nCU_FILE_ERROR_INVALID_VALUEon a invalid stream specification. \n\nCU_FILE_DRIVER_ERROR if the NVIDIA-fs driver cannot handle the request. \n\nCU_FILE_PLATFORM_NOT_SUPPORTED on unsupported platforms. \n\n \n\n\n\n4.4.2.\u00a0cuFileStreamDeregister\n\n\n\n\n\n\nCopy\nCopied!\n\n\n\n\n\n\n\n\n            \n            CUfileError_t cuFileStreamDeregister(CUStream_t stream);\n        \n\n\n\nParameters\n\n\n\nstreamCUDA stream in which to enqueue the operation. If NULL, make this operation in the default CUDA stream.  \n\nflagsReserved for future use. \n\nDescription\nThis optional API deregisters the stream with the cuFile subsystem.  \n         This API will free allocated cuFile resources associated with the stream. \n         The API will synchronize on the stream before releasing resources. \n         The stream pointer is expected to be a valid pointer. \n         The stream will be automatically deregistered as part of cuFileDriverClose.  \n          \n         Returns\n\n\nCU_FILE_SUCCESS on a successful submission. \n\nCU_FILE_ERROR_INVALID_VALUEon a invalid stream specification. \n\nCU_FILE_PLATFORM_NOT_SUPPORTED on unsupported platforms. \n\n \n\n\n\n4.4.3.\u00a0cuFileReadAsync\n\n\n\n\n\n\nCopy\nCopied!\n\n\n\n\n\n\n\n\n            \n            CUfileError_t cuFileReadAsync(CUFileHandle_t fh,\n                        void *bufPtr_base, \n                        size_t *size_p,\n                        off_t *file_offset_p, \n                        off_t *bufPtr_offset_p,\n                        int *bytes_read_p,\n                        CUstream stream);\n        \n\n\n\n\n\nEnqueues a read operation for the specified bytes from the cuFile handle into the device memory by using GDS functionality or to the host memory based on the type of memory pointer. \nIf non-NULL, the action is ordered in the stream.\n The current context of the caller is assumed. \nParameters\n\n\nfhThe cuFile handle for the file. \nbufPtr_base\n\nThe base address of the buffer in the memory into which to read. \nThe buffer can be allocated using either cudaMemory/cudaMallocHost/malloc/mmap. \nFor registered buffers, bufPtr_base must remain set to the base address used in cuFileBufRegister call. \n\n\nsize_pPointer to size in bytes to read. If the exact size is not known at the time of I/O submission, then you must set it to the maximum possible I/O size for that stream I/O.  \nfile_offset_pPointer to offset in the file from which to read. Unless otherwise set using cuFileStreamRegister API, this value will not be evaluated until execution time.  \nbufPtr_offset_pPointer to the offset relative to the bufPtr_base pointer from which to write. Unless otherwise set using cuFileStreamRegister API, this value will not be evaluated until execution time.  \nbytes_read_pPointer to the bytes read from the specified filehandle. This pointer should be a non NULL value and *bytes_read_p set to 0. After successful execution of the operation in the stream, the value *bytes_read_p will contain either: \n\nThe number of bytes successfully read.\n-1 on IO errors.\nAll other errors return a negative integer value of the CUfileOpError enum value. \n\n\nstream\n\nCUDA stream in which to enqueue the operation.\nIf NULL, make this operation synchronous.\n\n\n\nReturns\n\n\nCU_FILE_SUCCESS on a successful submission. \n\nCU_FILE_DRIVER_ERROR, if the nvidia-fs driver cannot handle the request. \n\nCU_FILE_ERROR_INVALID_VALUE on an input failure. \n\nCU_FILE_CUDA_ERROR on CUDA-specific errors. CUresult code can be obtained by using CU_FILE_CUDA_ERR(err).  \n\nDescription\n\nThis API reads the data from the specified file handle at the specified offset and size bytes into the GPU memory using GDS functionality. This is an asynchronous call and enqueues the operation into the specified CUDA stream and will not block the host thread for IO completion. The operation can be waited upon using cuStreamSynchronize(stream).  \nThe bytes_read_p memory should be allocated with cuMemHostAlloc/malloc/mmap or registered with cuMemHostRegister. The pointer to access that memory from the device can be obtained by using cuMemHostGetDevicePointer.  \nOperations that are enqueued with cuFile Stream APIs are FIFO ordered with respect to other work on the stream and must be completed before continuing to the next action in the stream. \nUnless otherwise specified through cuFileStreamRegister API, file offset, buffer offset or size parameter will not be evaluated until execution time. In these scenarios, size parameters should be set to the maximum possible I/O size at the time of submission and can be set to the actual size prior to the stream I/O execution. \n \n         \n          Refer to the following for more information: \n          \ncuFileRead\ncuFileWrite\ncuFileWriteAsync\n\n\n \n\n\n\n4.4.4.\u00a0cuFileWriteAsync\n\n\n\n\n\n\nCopy\nCopied!\n\n\n\n\n\n\n\n\n            \n            CUfileError_t cuFileWriteAsync(CUFileHandle_t fh,\n                        void *bufPtr_base, \n                        size_t *size_p,\n                        off_t file_offset_p, \n                        off_t bufPtr_offset_p,\n                        int *bytes_written_p,\n                        CUstream_t stream);\n        \n\n\n\n\n\nQueues Write operation for the specified bytes from the device memory into the cuFile handle by using GDS. \nParameters\n\n\nfhThe cuFile handle for the file. \nbufPtr_baseThe base address of the buffer in the memory from which to write. The buffer can be allocated using either cudaMemory/cudaMallocHost/malloc/mmap. For registered buffers, bufPtr_base must remain set to the base address used in the cuFileBufRegister call.  \nsize_pPointer to the size in bytes to write. If the exact size is not known at the time of I/O submission, then you must set it to the maximum possible I/O size for that stream I/O.  \nfile_offset_pPointer to the offset in the file from which to write. Unless otherwise set using cuFileStreamRegister API, this value will not be evaluated until execution time.  \nbufPtr_offset_pPointer to the offset relative to the bufPtr_base pointer from which to write. Unless otherwise set using cuFileStreamRegister API, this value will not be evaluated until execution time.  \nbytes_written_pPointer to the bytes written to the specified filehandle.This pointer should be a non NULL value and *bytes_written_p set to 0. After successful execution of the operation in the stream, the value *bytes_written_p will contain either: \n\nThe number of bytes successfully written.\n-1 on IO errors.\nAll other errors will return a negative integer value of the CUfileOpError enum value. \n\n\nstreamThe CUDA stream to enqueue the operation. \n\nReturns\n\n\nCU_FILE_SUCCESSon a successful submission. \n\nCU_FILE_DRIVER_ERROR, if the nvidia-fs driver cannot handle the request. \n\nCU_FILE_ERROR_INVALID_VALUE on an input failure. \n\nCU_FILE_CUDA_ERROR on CUDA-specific errors. The CUresult code can be obtained by using CU_FILE_CUDA_ERR(err).  \n\nDescription\n\nThis API writes the data from the GPU memory to a file specified by the file handle at a specified offset and size bytes by using GDS functionality. This is an asynchronous call and enqueues the operation into the specified CUDA stream and will not block the host thread for IO completion. The operation can be waited upon by using cuStreamSynchronize(stream). \nThe bytes_written pointer should be allocated with cuMemHostAlloc or registered with cuMemHostRegister, and the pointer to access that memory from the device can be obtained by using cuMemHostGetDevicePointer. \nOperations that are enqueued with cuFile Stream APIs are FIFO ordered with respect to other work on the stream and must be completed before continuing to the next action in the stream. \nUnless otherwise specified through cuFileStreamRegister API, file offset, buffer offset or size parameter will not be evaluated until execution time. In these scenarios, size parameters should be set to the maximum possible I/O size at the time of submission and can be set to the actual size prior to the stream I/O execution. \n \n         \n          See the following for more information: \n          \ncuFileRead\ncuFileWrite\ncuFileReadAsync\n\n\n\n \n\n\n\n4.5.\u00a0cuFile Batch API Functional Specification\n\n\n \n        \n        This section provides information about the cuFile Batch API functional specification.\n \n\n\n\n4.5.1.\u00a0cuFileBatchIOSetUp\n\n\n\n\n\n\nCopy\nCopied!\n\n\n\n\n\n\n\n\n            \n            CUfileError_t\ncuFileBatchIOSetUp(CUfileBatchHandle_t *batch_idp, int max_nr);\n        \n\n\n\nParameters\n\n\n\nmax_nr (Input) The maximum number of events this batch will hold. \n    \n Note:\n\n               The number should be between 1 - \u201cproperties.io_batch_size\u201d \n             \n\n \n\nbatch_idp(Output) Will be used in subsequent batch IO calls.  \n\nReturns\n\n\nCU_FILE_SUCCESS on success. \n\nCU_FILE_INTERNAL_ERROR on on any failures. \n\nDescription\nThis interface should be the first call in the sequence of batch I/O operation. This takes the maximum number of batch entries the caller intends to use and returns a CUFileBatchHandle_twhich should be used by the caller for subsequent batch I/O calls.  \n         \n          See the following for more information: \n          \ncuFileRead\ncuFileWrite\ncuFileReadAsync\ncuFileWriteAsync\ncuFileBatchIOGetStatus\ncuFileBatchIOCancel\ncuFileBatchIODestroy\n\n\n \n\n\n\n4.5.2.\u00a0cuFileBatchIOSubmit\n\n\n\n\n\n\nCopy\nCopied!\n\n\n\n\n\n\n\n\n            \n            CUfileError_t cuFileBatchIOSubmit(CUfileBatchHandle_t batch_idp,\n                                 unsigned nr, \n                                 CUfileIOParams_t *iocbp,\n                                 unsigned int flags)\n        \n\n\n\nParameters\n\n\nbatch_idpThe address of the output parameter for the newly created batch ID, which was obtained from a cuFileBatchSetup call.  \nnr\n\nThe number of requests for the batch request.\nThe value must be greater than 0 and less than or equal to max_nr specified in cuFileBatchIOSetup. \n\n\niocbpThe pointer contains the CUfileIOParams_t array structures of the length nrarray.  \nflagsReserved for future use. Should be set to 0. \n\nReturns\n\n\nCU_FILE_SUCCESS on success. \n\nCU_FILE_INTERNAL_ERROR on any failures. \n\nDescription\n\nThis API will need to be used to submit a read/write operation on an array of GPU/CPU data pointers from their respective file handle, offset, and size bytes. Based on the type of memory pointer, the data is transferred to/from the GPU memory by using GDS or the data is transferred to/from the CPU memory. \n\nThis is an asynchronous call and will enqueue the operation on a batch_id provided by the cuFileIOSetup API. The operation can be monitored when using this batch_id through cuFileBatchIOGetStatus. \nThe operation can be canceled by calling cuFileBatchIOCancel or destroyed by cuFileBatchIODestroy. \n \nThe entries in the CUfileIOParams_t array describe individual IOs. The bytes transacted field is valid only when the status indicates a completion.  \nOperations that are enqueued with cuFile Batch APIs are FIFO ordered with respect to other work on the stream and must be completed before continuing to the next action in the stream. Operations in each batch might be reordered with respect to each other. \nThe status field of individual IO operations via CUfileIOParams_t entries will have undefined values before the entire batch is complete. This definition is subject to change. \n \n         \n          See the following for more information: \n          \ncuFileRead\ncuFileWrite\ncuFileReadAsync\ncuFileWriteAsync\ncuFileBatchIOGetStatus\ncuFileBatchIOCancel\ncuFileBatchIODestroy\n\n\n \n\n\n\n4.5.3.\u00a0cuFileBatchIOGetStatus\n\n\n\n\n\n\nCopy\nCopied!\n\n\n\n\n\n\n\n\n            \n            CUfileError_t cuFileBatchIOGetStatus(CUfileBatchHandle_t batch_idp, \n                                     unsigned min_nr,\n                                     unsigned *nr,\n                                     CUfileIOEvents_t *iocbp,\n                                     struct timespec* timeout));\n        \n\n\n\nParameters\n\n\n\nbatch_idpObtained during setup. \n\nmin_nrThe minimum number of IO entries for which status is requested. The min_nr should be greater than or equal to zero and less than or equal to *nr.  \n\nnrThis is a pointer to max requested IO entries to poll for completion and is used as an Input/Output parameter. As an input *nr must be set to pass the maximum number of IO requests to poll for. As an output, *nr returns the number of completed I/Os.  \n\niocbp CUFileIOEvents_t array containing the status of completed I/Os in that batch.  \n\ntimeout This parameter is used to specify the amount of time to wait for in this API, even if the minimum number of requests have not completed. If the timeout hits, it is possible that the number of returned IOs can be less than min_nr.  \n\nReturns\n\n\nCU_FILE_SUCCESS on success. The success here refers to the completion of the API. Individual IO status and error can be obtained by examining the returned status and error in the array iocbp.  \n\nCU_FILE_ERROR_INVALID_VALUE for an invalid batch ID. \n\nDescription\n\nThis is a batch API to monitor the status of batch IO operations by using the batch_id that was returned by cuFileBatchIOSubmit. The operation will be canceled automatically if cuFileBatchIOCancel is called and the status will reflect CU_FILE_CANCELED for all canceled IO operations. \nThe status of each member of the batch is queried, which would not be possible with one CUEvent. The status field of individual IO operations via CUfileIOParams_t entries will have undefined values before the entire batch is completed. This definition is subject to change. \n \n         \n          See the following for more information: \n          \ncuFileBatchIOSubmit\ncuFileBatchIODestroy\n\n\n \n\n\n\n4.5.4.\u00a0cuFileBatchIOCancel\n\n\n\n\n\n\nCopy\nCopied!\n\n\n\n\n\n\n\n\n            \n            CUfileError_t cuFileBatchIOCancel(CUfileBatchHandle_t batch_idp)\n        \n\n\n\nParameters\n\n\n\nbatch_idpThe batch ID to cancel. \n\nReturns\n\n\nCU_FILE_SUCCESS on success. \n\nCU_FILE_ERROR_INVALID_VALUEon any failures. \n\nDescription\n\nThis is a batch API to cancel an ongoing IO batch operation by using the batch_id that was returned by cuFileBatchIOSubmit. This API tries to cancel an individual IO operation in the batch if possible and provides no guarantee about canceling an ongoing operation. \n \n         \n          Refer to the following for more information: \n          \ncuFileBatchIOGetStatus\ncuFileBatchIOSubmit\ncuFileBatchIODestroy\n\n\n \n\n\n\n4.5.5.\u00a0cuFileBatchIODestroy\n\n\n\n\n\n\nCopy\nCopied!\n\n\n\n\n\n\n\n\n            \n            void cuFileBatchIODestroy(CUfileBatchHandle_t batch_idp)\n        \n\n\n\nParameters\n\n\n\nbatch_idpThe batch handle to be destroyed. \n\nReturns\nvoid \n         Description\nThis is a batch API that destroys a batch context and the resources that are allocated with cuFileBatchIOSetup.  \n         \n          Refer to the following for more information: \n          \ncuFileBatchIOGetStatus\ncuFileBatchIOSubmit\ncuFileBatchIOCancel\n\n\n\n\n\n\n\n5. Sample Program with cuFile APIs\n\n\n \nThe following sample program uses the cuFile APIs:\n       \n\n\n\nCopy\nCopied!\n\n\n\n\n\n\n\n\n            \n            // To compile this sample code:\n//\n// nvcc gds_helloworld.cxx -o gds_helloworld -lcufile\n//\n// Set the environment variable TESTFILE\n// to specify the name of the file on a GDS enabled filesystem\n//\n// Ex:   TESTFILE=/mnt/gds/gds_test ./gds_helloworld\n//\n//\n#include <fcntl.h>\n#include <errno.h>\n#include <unistd.h>\n\n#include <cstdlib>\n#include <cstring>\n#include <iostream>\n#include <cuda_runtime.h>\n#include \"cufile.h\"\n\n//#include \"cufile_sample_utils.h\"\nusing namespace std;\n\nint main(void) {\n        int fd;\n        ssize_t ret;\n        void *devPtr_base;\n        off_t file_offset = 0x2000;\n        off_t devPtr_offset = 0x1000;\n        ssize_t IO_size = 1UL << 24;\n        size_t buff_size = IO_size + 0x1000;\n        CUfileError_t status;\n        // CUResult cuda_result;\n        int cuda_result;\n        CUfileDescr_t cf_descr;\n        CUfileHandle_t cf_handle;\n        char *testfn;\n        \n        testfn=getenv(\"TESTFILE\");\n        if (testfn==NULL) {\n            std::cerr << \"No testfile defined via TESTFILE.  Exiting.\" << std::endl;\n            return -1;\n        } \n       \n        cout << std::endl; \n        cout << \"Opening File \" << testfn << std::endl;\n\n        fd = open(testfn, O_CREAT|O_WRONLY|O_DIRECT, 0644);\n        if(fd < 0) {\n                std::cerr << \"file open \" << testfn << \"errno \" << errno << std::endl;\n                return -1;\n        }\n\n        // the above fd could also have been opened without O_DIRECT starting CUDA toolkit 12.2\n        // (gds 1.7.x version) as follows\n        // fd = open(testfn, O_CREAT|O_WRONLY, 0644);\n\n        cout << \"Opening cuFileDriver.\" << std::endl;\n        status = cuFileDriverOpen();\n        if (status.err != CU_FILE_SUCCESS) {\n                std::cerr << \" cuFile driver failed to open \" << std::endl;\n                close(fd);\n                return -1;\n        }\n\n        cout << \"Registering cuFile handle to \" << testfn << \".\" << std::endl;\n\n        memset((void *)&cf_descr, 0, sizeof(CUfileDescr_t));\n        cf_descr.handle.fd = fd;\n        cf_descr.type = CU_FILE_HANDLE_TYPE_OPAQUE_FD;\n        status = cuFileHandleRegister(&cf_handle, &cf_descr);\n        if (status.err != CU_FILE_SUCCESS) {\n                std::cerr << \"cuFileHandleRegister fd \" << fd << \" status \" << status.err << std::endl;\n                close(fd);\n                return -1;\n        }\n\n        cout << \"Allocating CUDA buffer of \" << buff_size << \" bytes.\" << std::endl;\n\n        cuda_result = cudaMalloc(&devPtr_base, buff_size);\n        if (cuda_result != CUDA_SUCCESS) {\n                std::cerr << \"buffer allocation failed \" << cuda_result << std::endl;\n                cuFileHandleDeregister(cf_handle);\n                close(fd);\n                return -1;\n        }\n\n        cout << \"Registering Buffer of \" << buff_size << \" bytes.\" << std::endl;\n        status = cuFileBufRegister(devPtr_base, buff_size, 0);\n        if (status.err != CU_FILE_SUCCESS) {\n                std::cerr << \"buffer registration failed \" << status.err << std::endl;\n                cuFileHandleDeregister(cf_handle);\n                close(fd);\n                cudaFree(devPtr_base);\n                return -1;\n        }\n\n        // fill a pattern\n        cout << \"Filling memory.\" << std::endl;\n\n        cudaMemset((void *) devPtr_base, 0xab, buff_size);\n        cuStreamSynchronize(0);\n\n        // perform write operation directly from GPU mem to file\n        cout << \"Writing buffer to file.\" << std::endl;\n        ret = cuFileWrite(cf_handle, devPtr_base, IO_size, file_offset, devPtr_offset);\n\n        if (ret < 0 || ret != IO_size) {\n                std::cerr << \"cuFileWrite failed \" << ret << std::endl;\n        }\n\n        // release the GPU memory pinning\n        cout << \"Releasing cuFile buffer.\" << std::endl;\n        status = cuFileBufDeregister(devPtr_base);\n        if (status.err != CU_FILE_SUCCESS) {\n                std::cerr << \"buffer deregister failed\" << std::endl;\n                cudaFree(devPtr_base);\n                cuFileHandleDeregister(cf_handle);\n                close(fd);\n                return -1;\n        }\n\n        cout << \"Freeing CUDA buffer.\" << std::endl;\n        cudaFree(devPtr_base);\n        // deregister the handle from cuFile\n        cout << \"Releasing file handle. \" << std::endl;\n        (void) cuFileHandleDeregister(cf_handle);\n        close(fd);\n\n        // release all cuFile resources\n        cout << \"Closing File Driver.\" << std::endl;\n        (void) cuFileDriverClose();\n\n        cout << std::endl; \n\n        return 0;\n}\n        \n\n\n\n\n\n\n\n\n6. Known Limitations of cuFile Batch APIs\n\n\n \n       \n       \n       This section provides information about the known limitations of cuFile Batch APIs in this release of GDS. \n\nBatch I/Os will be supported mainly by either the local file systems which are hosted on NVMe or NVMeOF devices or by the native file system that supports Linux AIO. Following table provides an overview of the cuFile batch API support with respect to different file systems. The following table provides an overview of cuFile batch API support with respect to distributed file systems: \n\n\n\n\nFile System\nGDS Batch Mode\nComments\n\n\n\n\nExt4/XFS \nRead/Write support\n\u00a0\n\n\nDDN EXAScaler\nRead/Write support\n\u00a0\n\n\nNFS\nRead/Write support\n\u00a0\n\n\nIBM Spectrum Scale \nNot available\nWill work in compat mode\n\n\nWeka\nNot available\nWill work in compat mode\n\n\nBeeGFS\nNot available\nWill work in compat mode\n\n\n\n\n   \n\n\n\n\nNotices\n\n\n \n \n\n\n\n\n\n\n    \n    Notice\n    \n\n This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (\u201cNVIDIA\u201d) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.\n NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.\n Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.\n NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (\u201cTerms of Sale\u201d). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document.\n NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer\u2019s own risk.\n NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer\u2019s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer\u2019s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.\n No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.\n Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.\n\n THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, \u201cMATERIALS\u201d) ARE BEING PROVIDED \u201cAS IS.\u201d NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA\u2019s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.\n\n \n\n\n\n\n\n\n\n    \n    OpenCL\n    \n\n\nOpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. \n        \n \n\n\n\n\n\n\n\n    \n    Trademarks\n    \n\n\nNVIDIA, the NVIDIA logo, DGX, DGX-1, DGX-2, DGX-A100, Tesla, and Quadro are trademarks and/or registered trademarks of NVIDIA Corporation in the United States and other countries. Other company and product names may be trademarks of the respective companies with which they are associated.  \n        \n\n\n\n\n\n\n\u00a9 2020-2024 NVIDIA Corporation and affiliates. All rights reserved.\nLast updated on Jun 24, 2024.\n\n\n\n\n\n\nTopics\n\n\n\n\n\n\n\n\n\n\n\n\nNVIDIA GPUDirect Storage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNVIDIA GPUDirect Storage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNVIDIA GPUDirect Storage Design Guide\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1. Introduction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2. Data Transfer Issues for GPU and Storage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3. GPUDirect Storage Benefits\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4. Application Suitability\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.1. Transfers To and From the GPU\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.2. Understanding IO Bottlenecks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.3. Explicit GDS APIs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.4. Pinned Memory for DMA Transfers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5. cuFile APIs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5. Platform Performance Suitability\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.1. Bandwidth from Storage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.2. Paths from Storage to GPUs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.3. GPU BAR1 Size\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6. Call to Action\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNVIDIA GPUDirect Storage Overview Guide\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1. Introduction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.1. Related Documents\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.2. Benefits for a Developer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.3. Intended Uses\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2. Functional Overview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.1. Explicit and Direct\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.2. Performance Optimizations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.2.1. Implementation Performance Enhancements\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.2.2. Concurrency Across Threads\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.2.3. Asynchrony\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.2.4. Batching\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.2.5. Use of CUDA Streams in cuFile\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.3. Compatibility and Generality\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.4. Monitoring\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.5. Scope of the Solutions in GDS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.6. Dynamic Routing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.6.1. cuFile Configuration for Dynamic Routing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.6.2. cuFile Configuration for DFS Mount\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.6.3. cuFile Configuration Validation for Dynamic Routing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3. Software Architecture\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.1. Software Components\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.2. Primary Components\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.1. Workflows for GDS Functionality\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.2. Workflow 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.3. Workflow 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3. Aligning with Other Linux Initiatives\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4. Deployment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.1. Software Components for Deployment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.2. Using GPUDirect Storage in Containers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncuFile API Reference Guide\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1. Introduction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2. Usage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.1. Dynamic Interactions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.2. Driver, File, and Buffer Management\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.3. cuFile Compatibility Mode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3. cuFile API Specification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.1. Data Types\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.1.1. Declarations and Definitions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.1.2. Typedefs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.1.3. Enumerations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.2. cuFile Driver APIs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3. cuFile Synchronous IO APIs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.4. cuFile File Handle APIs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.5. cuFile Buffer APIs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.6. cuFile Stream APIs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.7. cuFile Batch APIs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4. cuFile API Functional Specification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.1. cuFileDriver API Functional Specification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.1.1. cuFileDriverOpen\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.1.2. cuFileDriverClose\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.1.3. cuFileDriverGetProperties\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.1.4. cuFileDriverSetPollMode(bool poll, size_t poll_threshold_size)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.1.5. cuFileDriverSetMaxDirectIOSize(size_t max_direct_io_size)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.1.6. cuFileDriverSetMaxCacheSize(size_t max_cache_size)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.1.7. cuFileDriverSetMaxPinnedMemSize(size_t max_pinned_memory_size)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.2. cuFile IO API Functional Specification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.2.1. cuFileHandleRegister\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.2.2. cuFileHandleDeregister\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.2.3. cuFileRead\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.2.4. cuFileWrite\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.3. cuFile Memory Management Functional Specification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.3.1. cuFileBufRegister\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.3.2. cuFileBufDeregister\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.4. cuFile Stream API Functional Specification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.4.1. cuFileStreamRegister\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.4.2. cuFileStreamDeregister\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.4.3. cuFileReadAsync\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.4.4. cuFileWriteAsync\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5. cuFile Batch API Functional Specification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.1. cuFileBatchIOSetUp\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.2. cuFileBatchIOSubmit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.3. cuFileBatchIOGetStatus\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.4. cuFileBatchIOCancel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.5. cuFileBatchIODestroy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5. Sample Program with cuFile APIs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6. Known Limitations of cuFile Batch APIs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNVIDIA GPUDirect Storage Release Notes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1. Introduction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2. New Features and Changes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3. MLNX_OFED and Filesystem Requirements\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4. Support Matrix\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5. GDS Enabled Libraries/Frameworks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6. Included Packages\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7. Minor Updates and Bug Fixes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8. Known Issues\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9. Known Limitations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGetting Started with NVIDIA GPUDirect Storage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1. Introduction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2. If you are a system administrator or a performance engineer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3. If you are a developer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4. If you are OEM, ODM, CSP\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5. Troubleshooting GDS issues\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding GPUDirect Storage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNVIDIA GPUDirect Storage Best Practices Guide\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1. Introduction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2. Software Settings\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.1. System Settings\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.2. Use of CUDA Context in GPU Kernels and Storage IO\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.3. cuFile Configuration Settings\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3. API Usage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.1. cuFileDriverOpen\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.2. cuFileHandleRegister\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3. cuFileBufRegister, cuFileRead, cuFileWrite, cuFileBatchIOSubmit, cuFileBatchIOGetStatus, cuFileReadAsync, cuFileWriteAsync, and cuFileStreamRegister\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3.1. IO Pattern 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3.2. IO Pattern 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3.3. IO Pattern 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3.4. IO Pattern 4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3.5. IO Pattern 5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3.6. IO Pattern 6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3.7. IO Pattern 7\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.4. cuFileHandleDeregister\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.5. cuFileBufDeregister\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.6. cuFileStreamRegister\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.7. cuFileStreamDeregister\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.8. cuFileDriverClose\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNVIDIA GPUDirect Storage Benchmarking and Configuration Guide\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1. Introduction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2. About this Guide\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3. Benchmarking GPUDirect Storage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.1. Determining PCIe Device Affinity\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.2. GPUDirect Storage Configuration Parameters\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.1. System Parameters\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.2. GPUDirect Storage Parameters\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3. GPUDirect Storage Benchmarking Tools\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3.1. gdsio Utility\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3.2. gds-stats Tool\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4. GPUDirect Storage Benchmarking on Direct Attached Storage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.1. GPUDirect Storage Performance on DGX-2 System\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.2. GPUDirect Storage Performance on a DGX A100 System\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5. GPUDirect Storage Benchmarking on Network Attached Storage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.1. GPUDirect Storage Benchmarking on NFS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6. Summary\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA. Benchmarking and Performance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA.1. The Language of Performance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA.2. Benchmarking Storage Performance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNVIDIA GPUDirect Storage Installation and Troubleshooting Guide\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1. Introduction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2. Installing GPUDirect Storage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.1. Before You Install GDS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.2. Installing GDS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.2.1. Configuring File System Settings for GDS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.2.2. Verifying a Successful GDS Installation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.3. Installed GDS Libraries and Tools\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.4. Uninstalling GPUDirect Storage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.5. Environment Variables Used by GPUDirect Storage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.6. JSON Config Parameters Used by GPUDirect Storage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.7. GDS Configuration File Changes to Support Dynamic Routing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.8. Determining Which Version of GDS is Installed\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.9. Experimental Repos for Network Install of GDS Packages for DGX Systems\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3. API Errors\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.1. CU_FILE_DRIVER_NOT_INITIALIZED\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.2. CU_FILE_DEVICE_NOT_SUPPORTED\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3. CU_FILE_IO_NOT_SUPPORTED\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.4. CU_FILE_CUDA_MEMORY_TYPE_INVALID\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4. Basic Troubleshooting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.1. Log Files for the GDS Library\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.2. Enabling a Different cufile.log File for Each Application\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.3. Enabling Tracing GDS Library API Calls\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.4. cuFileHandleRegister Error\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5. Troubleshooting Applications that Return cuFile Errors\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.6. cuFile-* Errors with No Activity in GPUDirect Storage Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.7. CUDA Runtime and Driver Mismatch with Error Code 35\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.8. CUDA API Errors when Running the cuFile-* APIs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.9. Finding GDS Driver Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.10. Tracking IO Activity that Goes Through the GDS Driver\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.11. Read/Write Bandwidth and Latency Numbers in GDS Stats\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.12. Tracking Registration and Deregistration of GPU Buffers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.13. Enabling RDMA-specific Logging for Userspace File Systems\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.14. CUDA_ERROR_SYSTEM_NOT_READY After Installation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.15. Adding udev Rules for RAID Volumes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.16. When You Observe \"Incomplete write\" on NVME Drives\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.17. CUFILE async I/O is failing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5. Advanced Troubleshooting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.1. Resolving Hung cuFile* APIs with No Response\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.2. Sending Relevant Data to Customer Support\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.3. Resolving an IO Failure with EIO and Stack Trace Warning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.4. Controlling GPU BAR Memory Usage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.5. Determining the Amount of Cache to Set Aside\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.6. Monitoring BAR Memory Usage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.7. Resolving an ENOMEM Error Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.8. GDS and Compatibility Mode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.9. Enabling Compatibility Mode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.10. Tracking the IO After Enabling Compatibility Mode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.11. Bypassing GPUDirect Storage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.12. GDS Does Not Work for a Mount\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.13. Simultaneously Running the GPUDirect Storage IO and POSIX IO on the Same File\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.14. Running Data Verification Tests Using GPUDirect Storage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNVIDIA GPUDirect Storage Installation and Troubleshooting Guide\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6. Troubleshooting Performance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.1. Running Performance Benchmarks with GDS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.2. Tracking Whether GPUDirect Storage is Using an Internal Cache\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.3. Tracking when IO Crosses the PCIe Root Complex and Impacts Performance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.4. Using GPUDirect Statistics to Monitor CPU Activity\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.5. Monitoring Performance and Tracing with cuFile-* APIs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.6. Example: Using Linux Tracing Tools\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.7. Tracing the cuFile-* APIs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.8. Improving Performance using Dynamic Routing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7. Troubleshooting IO Activity\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.1. Managing Coherency of Data in the Page Cache and on Disk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8. EXAScaler Filesystem LNet Troubleshooting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8.1. Determining the EXAScaler Filesystem Client Module Version\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8.2. Checking the LNet Network Setup on a Client\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8.3. Checking the Health of the Peers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8.4. Checking for Multi-Rail Support\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8.5. Checking GDS Peer Affinity\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8.6. Checking for LNet-Level Errors\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8.7. Resolving LNet NIDs Health Degradation from Timeouts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8.8. Configuring LNet Networks with Multiple OSTs for Optimal Peer Selection\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9. Understanding EXAScaler Filesystem Performance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.1. osc Tuning Performance Parameters\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.2. Miscellaneous Commands for osc, mdc, and stripesize\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.3. Getting the Number of Configured Object-Based Disks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.4. Getting Additional Statistics related to the EXAScaler Filesystem\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.5. Getting Metadata Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.6. Checking for an Existing Mount\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.7. Unmounting an EXAScaler Filesystem Cluster\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.8. Getting a Summary of EXAScaler Filesystem Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.9. Using GPUDirect Storage in Poll Mode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10. Troubleshooting and FAQ for the WekaIO Filesystem\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.1. Downloading the WekaIO Client Package\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.2. Determining Whether the WekaIO Version is Ready for GDS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.3. Mounting a WekaIO File System Cluster\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.4. Resolving a Failing Mount\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.5. Resolving 100% Usage for WekaIO for Two Cores\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.6. Checking for an Existing Mount in the Weka File System\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.7. Checking for a Summary of the WekaIO Filesystem Status\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.8. Displaying the Summary of the WekaIO Filesystem Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.9. Why WekaIO Writes Go Through POSIX\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.10. Checking for nvidia-fs.ko Support for Memory Peer Direct\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.11. Checking Memory Peer Direct Stats\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.12. Checking for Relevant nvidia-fs Statistics for the WekaIO Filesystem\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.13. Conducting a Basic WekaIO Filesystem Test\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.14. Unmounting a WekaIO File System Cluster\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.15. Verify the Installed Libraries for the WekaIO Filesystem\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.16. GDS Configuration File Changes to Support the WekaIO Filesystem\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.17. Check for Relevant User-Space Statistics for the WekaIO Filesystem\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.18. Check for WekaFS Support\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n11. Enabling IBM Spectrum Scale Support with GDS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n11.1. IBM Spectrum Scale Limitations with GDS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n11.2. Checking nvidia-fs.ko Support for Mellanox PeerDirect\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n11.3. Verifying Installed Libraries for IBM Spectrum Scale\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n11.4. Checking PeerDirect Stats\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n11.5. Checking for Relevant nvidia-fs Stats with IBM Spectrum Scale\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n11.6. GDS User Space Stats for IBM Spectrum Scale for Each Process\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n11.7. GDS Configuration to Support IBM Spectrum Scale\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n11.8. Scenarios for Falling Back to Compatibility Mode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n11.9. GDS Limitations with IBM Spectrum Scale\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n12. NetApp E-series BeeGFS with GDS Solution Deployment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n12.1. Netapp BeeGFS/GPUDirect Storage and Package Requirements\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n12.2. BeeGFS Client Configuration for GDS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n12.3. GPU/HCA Topology on the Client - DGX-A100 and OSS servers Client Server\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n12.4. Verify the Setup\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n12.4.1. List the Management Node\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n12.4.2. List the Metadata Nodes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n12.4.3. List the Storage Nodes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n12.4.4. List the Client Nodes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n12.4.5. Display Client Connections\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n12.4.6. Verify Connectivity to the Different Services\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n12.4.7. List Storage Pools\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n12.4.8. Display the Free Space and inodes on the Storage and Metadata Targets\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n12.5. Testing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n12.5.1. Verifying Integration is Working\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n12.5.2. Conducting a Basic NetApp BeeGFS Filesystem Test\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n13. Setting Up and Troubleshooting VAST Data (NFSoRDMA+MultiPath)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n13.1. Installing MLNX_OFED and VAST NFSoRDMA+Multipath Packages\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n13.1.1. Client Software Requirements\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n13.1.2. Install the VAST Multipath Package\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n13.2. Set Up the Networking\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n13.2.1. VAST Network Configuration\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n13.2.2. Client Network Configuration\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n13.2.3. Verify Network Connectivity\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n13.3. Mount VAST NFS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n13.4. Debugging and Monitoring VAST Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n14. Troubleshooting and FAQ for NVMe and NVMeOF Support\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n14.1. MLNX_OFED Requirements and Installation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n14.2. Determining Whether the NVMe device is Supported for GDS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n14.3. RAID Support in GDS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n14.4. Mounting a Local Filesystem for GDS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n14.5. Check for an Existing EXT4 Mount\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n14.6. Check for IO Statistics with Block Device Mount\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n14.7. RAID Group Configuration for GPU Affinity\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n14.8. Conduct a Basic EXT4 Filesystem Test\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n14.9. Unmount a EXT4 Filesystem\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n14.10. Udev Device Naming for a Block Device\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n14.11. BATCH I/O Performance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15. Displaying GDS NVIDIA FS Driver Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15.1. nvidia-fs Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15.2. Analyze Statistics for each GPU\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15.3. Resetting the nvidia-fs Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15.4. Checking Peer Affinity Stats for a Kernel Filesystem and Storage Drivers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15.5. Checking the Peer Affinity Usage for a Kernel File System and Storage Drivers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15.6. Display the GPU-to-Peer Distance Table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15.7. The GDSIO Tool\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15.8. Tabulated Fields\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15.9. The GDSCHECK Tool\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15.10. NFS Support with GPUDirect Storage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15.10.1. Install Linux NFS server with RDMA Support on MLNX_OFED 5.3 or Later\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15.10.2. Install GPUDirect Storage Support for the NFS Client\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15.11. NFS GPUDirect Storage Statistics and Debugging\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15.12. GPUDirect Storage IO Behavior\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15.12.1. Read/Write Atomicity Consistency with GPUDirect Storage Direct IO\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15.12.2. Write with File a Opened in O_APPEND Mode (cuFileWrite)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15.12.3. GPU to NIC Peer Affinity\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15.12.4. Compatible Mode with Unregistered Buffers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15.12.5. Unaligned writes with Non-Registered Buffers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15.12.6. Process Hang with NFS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15.12.7. Tools Support Limitations for CUDA 9 and Earlier\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15.13. GDS Statistics for Dynamic Routing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15.13.1. Peer Affinity Dynamic Routing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15.13.2. cuFile Log Related to Dynamic Routing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n16. GDS Library Tracing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n16.1. Example: Display Tracepoints\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n16.1.1. Example: Tracepoint Arguments\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n16.2. Example: Track the IO Activity of a Process that Issues cuFileRead/ cuFileWrite\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n16.3. Example: Display the IO Pattern of all the IOs that Go Through GDS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n16.4. Understand the IO Pattern of a Process\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n16.5. IO Pattern of a Process with the File Descriptor on Different GPUs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n16.6. Determine the IOPS and Bandwidth for a Process in a GPU\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n16.7. Display the Frequency of Reads by Processes that Issue cuFileRead\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n16.8. Display the Frequency of Reads when cuFileRead Takes More than 0.1 ms\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n16.9. Displaying the Latency of cuFileRead for Each Process\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n16.10. Example: Tracking the Processes that Issue cuFileBufRegister\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n16.11. Example: Tracking Whether the Process is Constant when Invoking cuFileBufRegister\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n16.12. Example: Monitoring IOs that are Going Through the Bounce Buffer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n16.13. Example: Tracing cuFileRead and cuFileWrite Failures, Print, Error Codes, and Time of Failure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n16.14. Example: User-Space Statistics for Each GDS Process\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n16.15. Example: Viewing GDS User-Level Statistics for a Process\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n16.16. Example: Displaying Sample User-Level Statistics for each GDS Process\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n17. User-Space Counters in GPUDirect Storage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n17.1. Distribution of IO Usage in Each GPU\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n17.2. User-space Statistics for Dynamic Routing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n18. User-Space RDMA Counters in GPUDirect Storage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n18.1. cuFile RDMA IO Counters (PER_GPU RDMA STATS)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n18.2. cuFile RDMA Memory Registration Counters (RDMA MRSTATS)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n19. Cheat Sheet for Diagnosing Problems\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNVIDIA GPUDirect Storage O_DIRECT Requirements Guide\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1. Introduction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.1. Related Documents\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2. GPUDirect Storage Requirements\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.1. Summary of Basic Requirements\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.2. Client and Server\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.3. Cases Where O_DIRECT is Not a Fit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.3.1. Buffered IO\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.3.2. Inline Files\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.3.3. Block Allocation For Writes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.3.4. Examining or Transforming User Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.3.5. Summary\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCorporate Info\n\n\nNVIDIA.com Home\n\nAbout NVIDIA\n\n\n\n\n\n\n\n\u200eNVIDIA Developer\n\n\nDeveloper Home\n\nBlog\n\n\n\n\n\n\n\nResources\n\n\nContact Us\n\nDeveloper Program\n\n\n\n\n\n\n\n\n\n\n\n\nPrivacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | ContactCopyright \u00a9 2024 NVIDIA Corporation\n\n\n\n\n\n\n"}], "to_crawl": [["https://nvidia.github.io/cccl/thrust/", 1], ["https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html", 1], ["https://docs.nvidia.com/deploy/cuda-compatibility/index.html", 1], ["https://docs.nvidia.com/cupti/index.html", 1], ["https://docs.nvidia.com/gpudirect-storage/index.html", 1], ["https://docs.nvidia.com/compute-sanitizer/index.html", 1], ["https://docs.nvidia.com/nsight-systems/index.html", 1], ["https://docs.nvidia.com/nsight-compute/index.html", 1], ["https://docs.nvidia.com/nsight-visual-studio-edition/index.html", 1], ["https://developer.nvidia.com/cuda-toolkit-archive", 1], ["https://developer.nvidia.com/nvidia-video-codec-sdk", 1], ["https://developer.nvidia.com/nvidia-video-codec-sdk", 1], ["https://nvlabs.github.io/cub/", 1], ["https://nvidia.github.io/libcudacxx/", 1], ["https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html", 1], ["https://nvidia.github.io/cccl/thrust/", 1], ["https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html", 1], ["https://docs.nvidia.com/deploy/cuda-compatibility/index.html", 1], ["https://docs.nvidia.com/cupti/index.html", 1], ["https://docs.nvidia.com/gpudirect-storage/index.html", 1], ["https://docs.nvidia.com/compute-sanitizer/index.html", 1], ["https://docs.nvidia.com/nsight-systems/index.html", 1], ["https://docs.nvidia.com/nsight-compute/index.html", 1], ["https://docs.nvidia.com/nsight-visual-studio-edition/index.html", 1], ["https://www.nvidia.com/en-us/about-nvidia/privacy-policy/", 1], ["https://www.nvidia.com/en-us/about-nvidia/privacy-center/", 1], ["https://www.nvidia.com/en-us/preferences/start/", 1], ["https://www.nvidia.com/en-us/about-nvidia/terms-of-service/", 1], ["https://www.nvidia.com/en-us/about-nvidia/accessibility/", 1], ["https://www.nvidia.com/en-us/about-nvidia/company-policies/", 1], ["https://www.nvidia.com/en-us/product-security/", 1], ["https://www.nvidia.com/en-us/contact/", 1]]}